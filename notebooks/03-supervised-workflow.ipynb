{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import load_dataset, available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_dataset('lvq-pak', kind='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('lvq-pak', kind='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ds.data, ds.target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should save off train and test sets as part of the dataset process if you're planning on doing supervised learning...probable worth showing how to do this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an algorithm\n",
    "\n",
    "`make train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(random_state=42, max_iter=200000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it to predict\n",
    "`make predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = model.predict(X_test);\n",
    "p_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the quality of the prediction\n",
    "`make analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: \n",
    "* automate the basic workflow\n",
    "* compare a bunch of algorithms for our Swedish Chef paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: `make train`\n",
    "\n",
    "## Add our algorithm to available_algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import available_algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently no available algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(available_algorithms().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add an algorithm, add  a key:value pair to the dict `_ALGORITHMS` in `src/models/algorithms.py`.\n",
    "\n",
    "For example, add\n",
    "```\n",
    "'linearSVC': LinearSVC()\n",
    "```\n",
    "to the `_ALGORITHMS` dict, and add\n",
    "```\n",
    "from sklearn.svm import LinearSVC\n",
    "```\n",
    "to the top of the file.\n",
    "\n",
    "Also, add `linearSVC` to the docstring of `available_algorithms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(available_algorithms().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(available_algorithms.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in a position where the `make train` script can run using `linearSVC`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that `make train` takes a `models/model_list.json` as input. Let's make one.\n",
    "```\n",
    "## train / fit / build models\n",
    "train: models/model_list.json\n",
    "\t$(PYTHON_INTERPRETER) -m src.models.train_model model_list.json\n",
    "```\n",
    "\n",
    "A `model_list.json` is a list of dicts, where each dict specifices a combination of:\n",
    "* `dataset`: A valid dataset name from `available_datasets`\n",
    "* `dataset_params`: A dictionary of parameters that can be passed to `load_dataset()` with the specified `dataset`\n",
    "* `algorithm`: A valid dataset name from `available_algorithms`\n",
    "* `algorithm_params`: A dictionary of parameters to use when running the specified `algorithm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    {\n",
    "        'dataset': 'lvq-pak',\n",
    "        'dataset_params': {'kind': 'train'},\n",
    "        'algorithm': 'linearSVC',\n",
    "        'algorithm_params': {'random_state': 42, 'max_iter': 200000},\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import model_path\n",
    "from src.utils import save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(model_path / 'model_list.json', model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../models/model_list.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now running `make train` will train `LinearSVC` on `lvq-pak` with the specified parameters.\n",
    "\n",
    "The output will be:\n",
    "* A trained model in `models/trained_models`\n",
    "* A json file `models/trained_models.json` that keeps track of the models that we've trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Caching! Then, checking against existing files and metadata and looking for caching!\n",
    "\n",
    "## TODO: Don't overwrite the trained_models.json, append to it (as long as the files are still there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the output from `make train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import trained_model_path\n",
    "from src.data.utils import list_dir\n",
    "from src.utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_json(model_path / 'trained_models.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(load_json(model_path / 'trained_models.json').keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: have an \"available_trained_models()\" as function to access the results of this .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the trained model\n",
    "from src.models.train import load_model\n",
    "\n",
    "s_model, s_model_metadata = load_model(model_name='linearSVC_lvq-pak_0', model_path=trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: explore the effects of caching once it's implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any algorithm will work that: \n",
    "* is a subclass of the sklearn `BaseEstimator` class (needed for setting and getting params)\n",
    "* has a `fit` method (needed for `make train`)\n",
    "* has either a `predict` method (supervised) or a `transform` method (unsupervised) (needed for `make predict`)\n",
    "\n",
    "We will see how things work in the unsupervised case in the next example. \n",
    "\n",
    "Note that an **algorithm** here can be a combination of \"algorithms\" as long as that combination is a `BaseEstimator` with the above methods. For example, you can use an sklearn pipeline, or an sklearn meta estimator like GridSearchCV as an algorithm. \n",
    "\n",
    "If your algorithm of choice is **not yet** a `BaseEstimator` with the appropriate API, it is fairly easy to wrap it to be used in this way. While we won't have time to cover an example of this during the in-person part of this tutorial, the EDA Text Embedding (advanced usage tutorial) has an example of how to do this with gensim's FastText model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: `make predict`\n",
    "\n",
    "```\n",
    "## predict / transform / run experiments\n",
    "predict: models/predict_list.json\n",
    "\t$(PYTHON_INTERPRETER) -m src.models.predict_model predict_list.json\n",
    "```\n",
    "\n",
    "Similar to `models_list.json` in `predict_list.json` we specify the dataset to operate on, and in this case, the trained_model to apply to the given dataset.\n",
    "\n",
    "\n",
    "A `predict_list.json` is a list of dicts, where each dict specifices a combination of:\n",
    "* `dataset`: A valid dataset name from `available_datasets`\n",
    "* `dataset_params`: A dictionary of parameters that can be passed to `load_dataset()` with the specified `dataset`\n",
    "* `trained_model`: A valid dataset name from `available_trained_models` (aka. a key name in `trained_models.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bus_number]",
   "language": "python",
   "name": "conda-env-bus_number-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
