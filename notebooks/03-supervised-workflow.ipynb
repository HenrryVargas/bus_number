{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import load_dataset, available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall task: \n",
    "\n",
    "Train a supervised model on the lvq-pak. Try three different techniques, three times, and pick the one with the best accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_dataset('lvq-pak', kind='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('lvq-pak', kind='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ds.data, ds.target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should save off train and test sets as part of the dataset process if you're planning on doing supervised learning...probable worth showing how to do this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an algorithm\n",
    "\n",
    "Let's start with one algorithm!\n",
    "\n",
    "`make train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(random_state=42, max_iter=200000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it to predict\n",
    "`make predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = model.predict(X_test);\n",
    "p_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the quality of the prediction\n",
    "`make analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: \n",
    "* automate the basic workflow\n",
    "* compare 3 different algorithms run with 3 different random states for our Swedish Chef paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: `make train`\n",
    "\n",
    "## Add our algorithm to available_algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import available_algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently no available algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(available_algorithms().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add an algorithm, add  a key:value pair to the dict `_ALGORITHMS` in `src/models/algorithms.py`.\n",
    "\n",
    "For example, add\n",
    "```\n",
    "'linearSVC': LinearSVC()\n",
    "```\n",
    "to the `_ALGORITHMS` dict, and add\n",
    "```\n",
    "from sklearn.svm import LinearSVC\n",
    "```\n",
    "to the top of the file.\n",
    "\n",
    "Also, add `linearSVC` to the docstring of `available_algorithms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(available_algorithms().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(available_algorithms.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in a position where the `make train` script can run using `linearSVC`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that `make train` takes a `models/model_list.json` as input. Let's make one.\n",
    "```\n",
    "## train / fit / build models\n",
    "train: models/model_list.json\n",
    "\t$(PYTHON_INTERPRETER) -m src.models.train_model model_list.json\n",
    "```\n",
    "\n",
    "A `model_list.json` is a list of dicts, where each dict specifices a combination of:\n",
    "* `dataset_name`: A valid dataset name from `available_datasets`\n",
    "* `dataset_params`: A dictionary of parameters that can be passed to `load_dataset()` with the specified `dataset`\n",
    "* `algorithm_name`: A valid dataset name from `available_algorithms`\n",
    "* `algorithm_params`: A dictionary of parameters to use when running the specified `algorithm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    {\n",
    "        'dataset_name': 'lvq-pak',\n",
    "        'dataset_params': {'kind': 'train'},\n",
    "        'algorithm_name': 'linearSVC',\n",
    "        'algorithm_params': {'random_state': 42, 'max_iter': 200000},\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import model_path\n",
    "from src.utils import save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(model_path / 'model_list.json', model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../models/model_list.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now running `make train` will train `LinearSVC` on `lvq-pak` with the specified parameters.\n",
    "\n",
    "The output will be:\n",
    "* A trained model in `models/trained_models`\n",
    "* A json file `models/trained_models.json` that keeps track of the models that we've trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Caching! Then, checking against existing files and metadata and looking for caching! (note: will need a force parameter eventually)\n",
    "\n",
    "## TODO: Don't overwrite the trained_models.json, append to it (as long as the files are still there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the output from `make train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import trained_model_path\n",
    "from src.data.utils import list_dir\n",
    "from src.utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_json(model_path / 'trained_models.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(load_json(model_path / 'trained_models.json').keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: have an \"available_trained_models()\" as function to access the results of this .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the trained model\n",
    "from src.models.train import load_model\n",
    "\n",
    "s_model, s_model_metadata = load_model(model_name='linearSVC_lvq-pak_0', model_path=trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: explore the effects of caching once it's implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any algorithm will work that: \n",
    "* is a subclass of the sklearn `BaseEstimator` class (needed for setting and getting params)\n",
    "* has a `fit` method (needed for `make train`)\n",
    "* has either a `predict` method (supervised) or a `transform` method (unsupervised) (needed for `make predict`)\n",
    "\n",
    "We will see how things work in the unsupervised case in the next example. \n",
    "\n",
    "Note that an **algorithm** here can be a combination of \"algorithms\" as long as that combination is a `BaseEstimator` with the above methods. For example, you can use an sklearn pipeline, or an sklearn meta estimator like GridSearchCV as an algorithm. \n",
    "\n",
    "If your algorithm of choice is **not yet** a `BaseEstimator` with the appropriate API, it is fairly easy to wrap it to be used in this way. While we won't have time to cover an example of this during the in-person part of this tutorial, the EDA Text Embedding (advanced usage tutorial) has an example of how to do this with gensim's FastText model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: `make predict`\n",
    "\n",
    "```\n",
    "## predict / transform / run experiments\n",
    "predict: models/predict_list.json\n",
    "\t$(PYTHON_INTERPRETER) -m src.models.predict_model predict_list.json\n",
    "```\n",
    "\n",
    "Similar to `models_list.json` in `predict_list.json` we specify the dataset to operate on, and in this case, the trained_model to apply to the given dataset.\n",
    "\n",
    "\n",
    "A `predict_list.json` is a list of dicts, where each dict specifices a combination of:\n",
    "* `dataset_name`: A valid dataset name from `available_datasets`\n",
    "* `dataset_params`: A dictionary of parameters that can be passed to `load_dataset()` with the specified `dataset`\n",
    "* `model_name`: A valid dataset name from `available_trained_models` (aka. a key name in `trained_models.json`\n",
    "* `is_supervised`: Whether to use the `predict` (supervised) or `transform` (unsupervised) method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the test set here to do the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = [\n",
    "    {\n",
    "        'dataset_name': 'lvq-pak',\n",
    "        'dataset_params': {'kind': 'test'},\n",
    "        'model_name': 'linearSVC_lvq-pak_0',\n",
    "        'is_supervised': True\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(model_path / 'predict_list.json', predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO don't overwrite predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_json(model_path / 'predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.predict import load_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: available_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds = load_prediction('linearSVC_lvq-pak_0_exp_lvq-pak_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds.metadata['experiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: trained/tested on different data so far...this should work once the lvq-pak stuff is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds.data == p_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Move this function to a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import Dataset\n",
    "from src.paths import model_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_scorers():\n",
    "    _SCORERS = {\n",
    "        'accuracy_score': accuracy_score\n",
    "    }\n",
    "    return _SCORERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_score_df(predict_json='predictions.json', predict_json_path=None,\n",
    "                        score_list=['accuracy_score']):\n",
    "    if predict_json_path is None:\n",
    "        predict_json_path = model_path\n",
    "    else:\n",
    "        predict_json_path = pathlib.Path(predict_json_path)\n",
    "    predictions = load_json(predict_json_path / predict_json)\n",
    "    score_df = pd.DataFrame(columns=['score_name', 'algorithm_name', 'dataset_name',\n",
    "                                     'model_name', 'model_run_number'])\n",
    "    for current_scorer_name in score_list:\n",
    "        current_scorer = available_scorers()[current_scorer_name]\n",
    "\n",
    "        score_dict = {}\n",
    "        score_dict['score_name'] = current_scorer_name\n",
    "        for key in predictions.keys():\n",
    "            prediction = predictions[key]\n",
    "            exp = prediction['experiment']\n",
    "            pred_ds = load_prediction(prediction['dataset_name'], predict_path=model_output_path)\n",
    "\n",
    "            ds_name = exp['dataset_name']\n",
    "            ds_params = exp['dataset_params']\n",
    "            ds = load_dataset(ds_name, **ds_params)\n",
    "            score_dict['dataset_name'] = ds_name\n",
    "\n",
    "            score_dict['score'] = current_scorer(ds.target, pred_ds.data)\n",
    "\n",
    "            model_metadata = load_model(model_name=exp['model_name'], metadata_only=True)\n",
    "            score_dict['algorithm_name'] = model_metadata['algorithm_name']\n",
    "            score_dict['model_name'] = exp['model_name']\n",
    "            score_dict['model_run_number'] = exp['run_number']\n",
    "            new_score_df = pd.DataFrame(score_dict, index=[0])\n",
    "            score_df = score_df.append(new_score_df, sort=True)\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_score_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Add caching of the summary dfs to know if you're about to overwrite one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Add other algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add RandomForest and some other sklearn Classifier of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bus_number]",
   "language": "python",
   "name": "conda-env-bus_number-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
