{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are developing in a module, it's really handy to have these lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see debug-level logging in the notebook. Here's the incantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding and processing the Fashion-MNIST (FMNIST) Dataset\n",
    "\n",
    "A raw dataset is really just a list of files (and some useful metadata) that is later processed into a usable dataset. From a data provenance perspective, the most important things to know about raw data are:\n",
    "* Raw data is **hash-verified**. This ensures that if something changes upstream, we know about that change.\n",
    "* Raw data is **read only**, and is used to generate a separate and reproducible **Dataset** object\n",
    "* Raw data is **not saved** in the source code repository. (in fact, the whole `data` directory is specifically excluded in our `.gitignore`). Instead, the recipe for obtaining and processing the raw data is saved. (a snapshot of the raw data can be synced with a large data repo, like an AWS bucket, if desired.)\n",
    "\n",
    "Our approach to building a usable dataset is:\n",
    "\n",
    "1. Assemble the raw data files. Generate (and record) hashes to ensure the validity of these files.\n",
    "3. Add LICENSE and DESCR (description) metadata to make the raw data usable for other people, and\n",
    "4. Write a function to process the raw data into a usable format (for us, a `Dataset` object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"f-mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original MNIST dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "The dataset is free to use under an MIT license.\n",
    "\n",
    "It can be found online at https://github.com/zalandoresearch/fashion-mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directories, `paths` and `pathlib`\n",
    "\n",
    "Recall from our `README.md` the locations of our data files\n",
    "\n",
    "* `data`\n",
    "    * Data directory. often symlinked to a filesystem with lots of space\n",
    "    * `data/raw` \n",
    "        * Raw (immutable) hash-verified downloads\n",
    "    * `data/interim` \n",
    "        * Extracted and interim data representations, such as caches\n",
    "    * `data/processed` \n",
    "        * The final, cleaned and processed data sets for modeling.\n",
    "\n",
    "However, we **do not want to hardcode these paths** in our scripts.  This is what our `src.paths` module is for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import raw_data_path, interim_data_path, processed_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick aside: Use `pathlib`! Read more here:https://realpython.com/python-pathlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes exploring from a notebook a bit easier\n",
    "from src.utils import list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{raw_data_path}\")\n",
    "list_dir(raw_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Check Hashes\n",
    "The next step is to fetch these files and check (or generate) their hashes. The object we use to house this information is a `RawDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the FMNIST GitHub documentation, we see that the raw data is distributed as a set of 4 files. Because Zalando are excellent data citizens, they have conveniently given us MD5 hashes that we can verify when we download this data.\n",
    "\n",
    "| Name  | Content | Examples | Size | Link | MD5 Checksum|\n",
    "| --- | --- |--- | --- |--- |--- |\n",
    "| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n",
    "| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n",
    "| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n",
    "| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the raw files  and their hashes\n",
    "data_site = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com'\n",
    "file_list = [\n",
    "    ('train-images-idx3-ubyte.gz','8d4fb7e6c68d591d4c3dfef9ec88bf0d'),\n",
    "    ('train-labels-idx1-ubyte.gz','25c81989df183df01b3e8a0aad5dffbe'),\n",
    "    ('t10k-images-idx3-ubyte.gz', 'bef4ecab320f06d8554ea6380940ec79'),\n",
    "    ('t10k-labels-idx1-ubyte.gz', 'bb300cfdad3c16e7a12a480ee83cd310'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = RawDataset(dataset_name)\n",
    "for file, hashval in file_list:\n",
    "    url = f\"{data_site}/{file}\"\n",
    "    fmnist.add_url(url=url, hash_type='md5', hash_value=hashval)\n",
    "# Download and check the hashes\n",
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add License and Description\n",
    "Before we can turn this raw data into a usable dataset, we need to know 2 things:\n",
    "1. What does the raw data look like? Where did I get it from? What format is it in? What should it look like when it's processed? (DESCR)\n",
    "2. Am I allowed to use this data? (LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy case. Zalando are good data citizens, so their data License is directly available from\n",
    "# their Raw Data Repo on github\n",
    "\n",
    "# Notice we tag this data with the name `LICENSE`\n",
    "fmnist.add_url(url='https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/LICENSE',\n",
    "            name='LICENSE', file_name=f'{dataset_name}.license')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the raw data look like?\n",
    "# Where did I get it from? \n",
    "# What format is it in?\n",
    "# What should it look like when it's processed?\n",
    "fmnist_readme = '''\n",
    "Fashion-MNIST\n",
    "=============\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 70000\n",
    "    :Number of Attributes: 728\n",
    "    :Attribute Information: 28x28 8-bit greyscale image\n",
    "    :Missing Attribute Values: None\n",
    "    :Creator: Zalando\n",
    "    :Date: 2017\n",
    "\n",
    "This is a copy of Zalando's Fashion-MNIST [F-MNIST] dataset:\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original [MNIST] dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "References\n",
    "----------\n",
    "  - [F-MNIST] Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\n",
    "    Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv:1708.07747\n",
    "  - [MNIST] The MNIST Database of handwritten digits. Yann LeCun, Corinna Cortes,\n",
    "    Christopher J.C. Burges. http://yann.lecun.com/exdb/mnist/\n",
    "'''\n",
    "\n",
    "fmnist.add_metadata(kind=\"DESCR\", contents=fmnist_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fmnist.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it a second time. note it is cached!\n",
    "ds = fmnist.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)\n",
    "print(ds.LICENSE)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a `RawDataset` into a usable `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a `Dataset` object?\n",
    "It's a scikit-learn-style `Bunch`, containing:\n",
    "* data: the processed data\n",
    "* target: (optional) target vector (for supervised learning problems)\n",
    "* metadata: Data about the data\n",
    "\n",
    "Under the hood, this is basically a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, `data` and `target` are empty, since we haven't given any indication how to process the raw files into usable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the raw files into `data` and `target`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we turn these raw files into processed data?\n",
    "\n",
    "First, we need to unpack them. Fortunately, the `RawDataset` object knows how to unpack or decompress most common archive formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_dir = fmnist.unpack()\n",
    "print(unpack_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, raw data is unpacked to `interim_data_path` in a directory corresponding to the `dataset_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(interim_data_path / dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
    "we see how to process this data. H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the raw data\n",
    "Finally, we need to convert the raw data into usable `data` and `target` vectors.\n",
    "The code at https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py tells us how to do that. Having a look at the sample code, we notice that we need numpy. How do we add this to the environment?\n",
    "* Add it to `environment.yml`\n",
    "* `make requirements`\n",
    "\n",
    "Once we have done this, we can do a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "unpack_path = fmnist.unpack()\n",
    "kind = \"train\"\n",
    "\n",
    "label_path = unpack_path / f\"{kind}-labels-idx1-ubyte\"\n",
    "with open(label_path, 'rb') as fd:\n",
    "    target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "dataset_path = unpack_path / f\"{kind}-images-idx3-ubyte\"\n",
    "with open(dataset_path, 'rb') as fd:\n",
    "    data = np.frombuffer(fd.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)\n",
    "\n",
    "print(f'Data: {data.shape}, Target: {target.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a `Dataset`\n",
    "A Processing function produces a dictionary of kwargs that can be used as a `Dataset` constructor:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset\n",
    "help(Dataset.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, a processing function must should accept `dataset_name` and `metadata` keywords. Any additional metadata should be added to the `metadata` object that is passed in.\n",
    "\n",
    "\n",
    "Rewriting the sample code into this framework gives us this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file -a ../src/data/localdata.py\n",
    "#__all__ += ['process_mnist']\n",
    "\n",
    "def process_mnist(dataset_name='mnist', kind='train', metadata=None):\n",
    "    '''\n",
    "    Load the MNIST dataset (or a compatible variant; e.g. F-MNIST)\n",
    "\n",
    "    dataset_name: {'mnist', 'f-mnist'}\n",
    "        Which variant to load\n",
    "    kind: {'train', 'test'}\n",
    "        Dataset comes pre-split into training and test data.\n",
    "        Indicates which dataset to load\n",
    "    metadata: dict\n",
    "        Additional metadata fields will be added to this dict.\n",
    "        'kind': value of `kind` used to generate a subset of the data\n",
    "    '''\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "        \n",
    "    if kind == 'test':\n",
    "        kind = 't10k'\n",
    "\n",
    "    label_path = interim_data_path / dataset_name / f\"{kind}-labels-idx1-ubyte\"\n",
    "    with open(label_path, 'rb') as fd:\n",
    "        target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "    dataset_path = interim_data_path / dataset_name / f\"{kind}-images-idx3-ubyte\"\n",
    "    with open(dataset_path, 'rb') as fd:\n",
    "        data = np.frombuffer(fd.read(), dtype=np.uint8,\n",
    "                                       offset=16).reshape(len(target), 784)\n",
    "    metadata['subset'] = kind\n",
    "    \n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "        'metadata': metadata,\n",
    "    }\n",
    "    return dset_opts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.data.localdata import process_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.unpack(force=True)\n",
    "fmnist.load_function = partial(process_mnist, dataset_name='f-mnist')\n",
    "ds = fmnist.process(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.shape, ds.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding this Dataset to the master dataset list\n",
    "All the commands for manipulating the various lists of activities in our data science project are in a module called `workflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Raw Dataset to the master list of Raw Datasets\n",
    "workflow.add_raw_dataset(fmnist)\n",
    "workflow.available_raw_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pair of Datasets from this Raw Dataset, by specifying different options for the RawDataset creation\n",
    "for kind in ['train', 'test']:\n",
    "    workflow.add_transformer(from_raw=fmnist.name, raw_dataset_opts={'kind':kind}, \n",
    "                             output_dataset=f\"{fmnist.name}_{kind}\")\n",
    "\n",
    "workflow.get_transformer_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transforms and save the resulting Datasets. This is the same as doing a `make transform_data`\n",
    "logger.setLevel(logging.INFO)\n",
    "workflow.apply_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load these datsets by name:\n",
    "ds = Dataset.load(\"f-mnist_test\")\n",
    "print(f\"Data:{ds.data.shape}, Target:{ds.target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load(\"f-mnist_train\")\n",
    "print(f\"Data:{ds.data.shape}, Target:{ds.target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check in the new `raw_datasets.json`, `transformer_list.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check the generated `datasets.json` in to source code control\n",
    "* do a `make data`\n",
    "\n",
    "Your project now generates its usable datasets from Raw files. **Welcome to reproducible data science!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
