{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are developing in a module, it's really handy to have these lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see debug-level logging in the notebook. Here's the incantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding and processing the Fashion-MNIST (FMNIST) Dataset\n",
    "\n",
    "A raw dataset is really just a list of files (and some useful metadata) that is later processed into a usable dataset. From a data provenance perspective, the most important things to know about raw data are:\n",
    "* Raw data is **hash-verified**. This ensures that if something changes upstream, we know about that change.\n",
    "* Raw data is **read only**, and is used to generate a separate and reproducible **Dataset** object\n",
    "* Raw data is **not saved** in the source code repository. (in fact, the whole `data` directory is specifically excluded in our `.gitignore`). Instead, the recipe for obtaining and processing the raw data is saved. (a snapshot of the raw data can be synced with a large data repo, like an AWS bucket, if desired.)\n",
    "\n",
    "Our approach to building a usable dataset is:\n",
    "\n",
    "1. Assemble the raw data files. Generate (and record) hashes to ensure the validity of these files.\n",
    "3. Add LICENSE and DESCR (description) metadata to make the raw data usable for other people, and\n",
    "4. Write a function to process the raw data into a usable format (for us, a `Dataset` object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"f-mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original MNIST dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "The dataset is free to use under an MIT license.\n",
    "\n",
    "It can be found online at https://github.com/zalandoresearch/fashion-mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directories, `paths` and `pathlib`\n",
    "\n",
    "Recall from our `README.md` the locations of our data files\n",
    "\n",
    "* `data`\n",
    "    * Data directory. often symlinked to a filesystem with lots of space\n",
    "    * `data/raw` \n",
    "        * Raw (immutable) hash-verified downloads\n",
    "    * `data/interim` \n",
    "        * Extracted and interim data representations, such as caches\n",
    "    * `data/processed` \n",
    "        * The final, cleaned and processed data sets for modeling.\n",
    "\n",
    "However, we **do not want to hardcode these paths** in our scripts.  This is what our `src.paths` module is for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import raw_data_path, interim_data_path, processed_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick aside: Use `pathlib`! Read more here:https://realpython.com/python-pathlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes exploring from a notebook a bit easier\n",
    "from src.data.utils import list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ava00088/src/devel/bus_number/data/raw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f-mnist.readme',\n",
       " 'train-labels-idx1-ubyte.gz',\n",
       " 't10k-labels-idx1-ubyte.gz',\n",
       " 't10k-images-idx3-ubyte.gz',\n",
       " 'train-images-idx3-ubyte.gz',\n",
       " 'f-mnist.license']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{raw_data_path}\")\n",
    "list_dir(raw_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Check Hashes\n",
    "The next step is to fetch these files and check (or generate) their hashes. The object we use to house this information is a `RawDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the FMNIST GitHub documentation, we see that the raw data is distributed as a set of 4 files. Because Zalando are excellent data citizens, they have conveniently given us MD5 hashes that we can verify when we download this data.\n",
    "\n",
    "| Name  | Content | Examples | Size | Link | MD5 Checksum|\n",
    "| --- | --- |--- | --- |--- |--- |\n",
    "| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n",
    "| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n",
    "| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n",
    "| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the raw files  and their hashes\n",
    "data_site = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com'\n",
    "file_list = [\n",
    "    ('train-images-idx3-ubyte.gz','8d4fb7e6c68d591d4c3dfef9ec88bf0d'),\n",
    "    ('train-labels-idx1-ubyte.gz','25c81989df183df01b3e8a0aad5dffbe'),\n",
    "    ('t10k-images-idx3-ubyte.gz', 'bef4ecab320f06d8554ea6380940ec79'),\n",
    "    ('t10k-labels-idx1-ubyte.gz', 'bb300cfdad3c16e7a12a480ee83cd310'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.logging:No file_name specified. Inferring train-images-idx3-ubyte.gz from URL\n",
      "DEBUG:src.logging:train-images-idx3-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring train-labels-idx1-ubyte.gz from URL\n",
      "DEBUG:src.logging:train-labels-idx1-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring t10k-images-idx3-ubyte.gz from URL\n",
      "DEBUG:src.logging:t10k-images-idx3-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring t10k-labels-idx1-ubyte.gz from URL\n",
      "DEBUG:src.logging:t10k-labels-idx1-ubyte.gz already exists and hash is valid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist = RawDataset(dataset_name)\n",
    "for file, hashval in file_list:\n",
    "    url = f\"{data_site}/{file}\"\n",
    "    fmnist.add_url(url=url, hash_type='md5', hash_value=hashval)\n",
    "# Download and check the hashes\n",
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f-mnist.readme',\n",
       " 'train-labels-idx1-ubyte.gz',\n",
       " 't10k-labels-idx1-ubyte.gz',\n",
       " 't10k-images-idx3-ubyte.gz',\n",
       " 'train-images-idx3-ubyte.gz',\n",
       " 'f-mnist.license']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dir(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add License and Description\n",
    "Before we can turn this raw data into a usable dataset, we need to know 2 things:\n",
    "1. What does the raw data look like? Where did I get it from? What format is it in? What should it look like when it's processed? (DESCR)\n",
    "2. Am I allowed to use this data? (LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy case. Zalando are good data citizens, so their data License is directly available from\n",
    "# their Raw Data Repo on github\n",
    "\n",
    "# Notice we tag this data with the name `LICENSE`\n",
    "fmnist.add_url(url='https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/LICENSE',\n",
    "            name='LICENSE', file_name=f'{dataset_name}.license')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the raw data look like?\n",
    "# Where did I get it from? \n",
    "# What format is it in?\n",
    "# What should it look like when it's processed?\n",
    "fmnist_readme = '''\n",
    "Fashion-MNIST\n",
    "=============\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 70000\n",
    "    :Number of Attributes: 728\n",
    "    :Attribute Information: 28x28 8-bit greyscale image\n",
    "    :Missing Attribute Values: None\n",
    "    :Creator: Zalando\n",
    "    :Date: 2017\n",
    "\n",
    "This is a copy of Zalando's Fashion-MNIST [F-MNIST] dataset:\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original [MNIST] dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "References\n",
    "----------\n",
    "  - [F-MNIST] Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\n",
    "    Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv:1708.07747\n",
    "  - [MNIST] The MNIST Database of handwritten digits. Yann LeCun, Corinna Cortes,\n",
    "    Christopher J.C. Burges. http://yann.lecun.com/exdb/mnist/\n",
    "'''\n",
    "\n",
    "fmnist.add_metadata(kind=\"DESCR\", contents=fmnist_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.logging:No file_name specified. Inferring train-images-idx3-ubyte.gz from URL\n",
      "DEBUG:src.logging:train-images-idx3-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring train-labels-idx1-ubyte.gz from URL\n",
      "DEBUG:src.logging:train-labels-idx1-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring t10k-images-idx3-ubyte.gz from URL\n",
      "DEBUG:src.logging:t10k-images-idx3-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring t10k-labels-idx1-ubyte.gz from URL\n",
      "DEBUG:src.logging:t10k-labels-idx1-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:f-mnist.license exists, but no hash to check. Setting to sha1:a8a7a35b62521386e849ce242bdc89964e177b12\n",
      "DEBUG:src.logging:Creating f-mnist.readme from `contents` string\n",
      "DEBUG:src.logging:f-mnist.readme exists, but no hash to check. Setting to sha1:db57a3964b6b3515901f665412297aabf69e007e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.logging:Ungzipping train-images-idx3-ubyte\n",
      "INFO:src.logging:Ungzipping train-labels-idx1-ubyte\n",
      "INFO:src.logging:Ungzipping t10k-images-idx3-ubyte\n",
      "INFO:src.logging:Ungzipping t10k-labels-idx1-ubyte\n",
      "INFO:src.logging:Copying f-mnist.license\n",
      "INFO:src.logging:Copying f-mnist.readme\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ava00088/src/devel/bus_number/data/interim/f-mnist')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist.unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.logging:Found cached Dataset for f-mnist: d95a6db563698fce9ad2afc908c56a11c7693ade\n"
     ]
    }
   ],
   "source": [
    "ds = fmnist.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.logging:Found cached Dataset for f-mnist: d95a6db563698fce9ad2afc908c56a11c7693ade\n"
     ]
    }
   ],
   "source": [
    "# do it a second time. note it is cached!\n",
    "ds = fmnist.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fashion-MNIST\n",
      "=============\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 70000\n",
      "    :Number of Attributes: 728\n",
      "    :Attribute Information: 28x28 8-bit greyscale image\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: Zalando\n",
      "    :Date: 2017\n",
      "\n",
      "This is a copy of Zalando's Fashion-MNIST [F-MNIST] dataset:\n",
      "https://github.com/zalandoresearch/fashion-mnist\n",
      "\n",
      "Fashion-MNIST is a dataset of Zalando's article images—consisting of a\n",
      "training set of 60,000 examples and a test set of 10,000\n",
      "examples. Each example is a 28x28 grayscale image, associated with a\n",
      "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
      "drop-in replacement for the original [MNIST] dataset for benchmarking\n",
      "machine learning algorithms. It shares the same image size and\n",
      "structure of training and testing splits.\n",
      "\n",
      "References\n",
      "----------\n",
      "  - [F-MNIST] Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\n",
      "    Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv:1708.07747\n",
      "  - [MNIST] The MNIST Database of handwritten digits. Yann LeCun, Corinna Cortes,\n",
      "    Christopher J.C. Burges. http://yann.lecun.com/exdb/mnist/\n",
      "\n",
      "The MIT License (MIT) Copyright © 2017 Zalando SE, https://tech.zalando.com\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
      "\n",
      "<Dataset: f-mnist, data.shape=(60000, 784), target.shape=(60000,), metadata=['license', 'descr', 'dataset_name', 'subset', 'hash_type', 'data_hash', 'target_hash']>\n"
     ]
    }
   ],
   "source": [
    "print(ds.DESCR)\n",
    "print(ds.LICENSE)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a `RawDataset` into a usable `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a `Dataset` object?\n",
    "It's a scikit-learn-style `Bunch`, containing:\n",
    "* data: the processed data\n",
    "* target: (optional) target vector (for supervised learning problems)\n",
    "* metadata: Data about the data\n",
    "\n",
    "Under the hood, this is basically a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, `data` and `target` are empty, since we haven't given any indication how to process the raw files into usable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the raw files into `data` and `target`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we turn these raw files into processed data?\n",
    "\n",
    "First, we need to unpack them. Fortunately, the `RawDataset` object knows how to unpack or decompress most common archive formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.logging:Raw Dataset f-mnist is already unpacked. Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ava00088/src/devel/bus_number/data/interim/f-mnist\n"
     ]
    }
   ],
   "source": [
    "unpack_dir = fmnist.unpack()\n",
    "print(unpack_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, raw data is unpacked to `interim_data_path` in a directory corresponding to the `dataset_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train-labels-idx1-ubyte',\n",
       " 'f-mnist.readme',\n",
       " 'train-images-idx3-ubyte',\n",
       " 't10k-images-idx3-ubyte',\n",
       " 't10k-labels-idx1-ubyte',\n",
       " 'f-mnist.license']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dir(interim_data_path / dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
    "we see how to process this data. H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the raw data\n",
    "Finally, we need to convert the raw data into usable `data` and `target` vectors.\n",
    "The code at https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py tells us how to do that. Having a look at the sample code, we notice that we need numpy. How do we add this to the environment?\n",
    "* Add it to `environment.yml`\n",
    "* `make requirements`\n",
    "\n",
    "Once we have done this, we can do a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.logging:Raw Dataset f-mnist is already unpacked. Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (60000, 784), Target: (60000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unpack_path = fmnist.unpack()\n",
    "kind = \"train\"\n",
    "\n",
    "label_path = unpack_path / f\"{kind}-labels-idx1-ubyte\"\n",
    "with open(label_path, 'rb') as fd:\n",
    "    target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "dataset_path = unpack_path / f\"{kind}-images-idx3-ubyte\"\n",
    "with open(dataset_path, 'rb') as fd:\n",
    "    data = np.frombuffer(fd.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)\n",
    "\n",
    "print(f'Data: {data.shape}, Target: {target.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a `Dataset`\n",
    "A Processing function produces a dictionary of kwargs that can be used as a `Dataset` constructor:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module src.data.dset:\n",
      "\n",
      "__init__(self, dataset_name=None, data=None, target=None, metadata=None, license_txt=None, descr_txt=None, license_file=None, descr_file=None, **kwargs)\n",
      "    Object representing a dataset object.\n",
      "    Notionally compatible with scikit-learn's Bunch object\n",
      "    \n",
      "    dataset_name: string (required)\n",
      "        key to use for this dataset\n",
      "    data:\n",
      "        Data: (usually np.array or np.ndarray)\n",
      "    target: np.array\n",
      "        Either classification target or label to be used. for each of the points\n",
      "        in `data`\n",
      "    metadata: dict\n",
      "        Data about the object. Key fields include `license_txt` and `descr`\n",
      "    license_txt: str\n",
      "        String to use as the LICENSE for this dataset\n",
      "    license_file: filename\n",
      "        If `license_txt` is None, license text can be read from this file\n",
      "    descr_txt: str\n",
      "        String to use as the DESCR (description) for this dataset\n",
      "    descr_file: filename\n",
      "        If `descr_txt` is None, description text can be read from this file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.data import Dataset\n",
    "help(Dataset.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, a processing function must should accept `dataset_name` and `metadata` keywords. Any additional metadata should be added to the `metadata` object that is passed in.\n",
    "\n",
    "\n",
    "Rewriting the sample code into this framework gives us this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/data/localdata.py\n"
     ]
    }
   ],
   "source": [
    "%%file ../src/data/localdata.py\n",
    "__all__ = ['process_mnist']\n",
    "\n",
    "from ..paths import interim_data_path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def process_mnist(dataset_name='mnist', kind='train', metadata=None):\n",
    "    '''\n",
    "    Load the MNIST dataset (or a compatible variant; e.g. F-MNIST)\n",
    "\n",
    "    dataset_name: {'mnist', 'f-mnist'}\n",
    "        Which variant to load\n",
    "    kind: {'train', 'test'}\n",
    "        Dataset comes pre-split into training and test data.\n",
    "        Indicates which dataset to load\n",
    "    metadata: dict\n",
    "        Additional metadata fields will be added to this dict.\n",
    "        'kind': value of `kind` used to generate a subset of the data\n",
    "    '''\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "        \n",
    "    if kind == 'test':\n",
    "        kind = 't10k'\n",
    "\n",
    "    label_path = interim_data_path / dataset_name / f\"{kind}-labels-idx1-ubyte\"\n",
    "    with open(label_path, 'rb') as fd:\n",
    "        target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "    dataset_path = interim_data_path / dataset_name / f\"{kind}-images-idx3-ubyte\"\n",
    "    with open(dataset_path, 'rb') as fd:\n",
    "        data = np.frombuffer(fd.read(), dtype=np.uint8,\n",
    "                                       offset=16).reshape(len(target), 784)\n",
    "    metadata['subset'] = kind\n",
    "    \n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "        'metadata': metadata,\n",
    "    }\n",
    "    return dset_opts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.data.localdata import process_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.logging:Ungzipping train-images-idx3-ubyte\n",
      "INFO:src.logging:Ungzipping train-labels-idx1-ubyte\n",
      "INFO:src.logging:Ungzipping t10k-images-idx3-ubyte\n",
      "INFO:src.logging:Ungzipping t10k-labels-idx1-ubyte\n",
      "INFO:src.logging:Copying f-mnist.license\n",
      "INFO:src.logging:Copying f-mnist.readme\n",
      "DEBUG:src.logging:Wrote d95a6db563698fce9ad2afc908c56a11c7693ade.metadata\n",
      "DEBUG:src.logging:Wrote d95a6db563698fce9ad2afc908c56a11c7693ade.dataset\n"
     ]
    }
   ],
   "source": [
    "fmnist.unpack(force=True)\n",
    "fmnist.load_function = partial(process_mnist, dataset_name='f-mnist')\n",
    "ds = fmnist.process(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data.shape, ds.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding this Dataset to the master dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import add_dataset, load_dataset, available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dataset(fmnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.logging:No file_name specified. Inferring train-images-idx3-ubyte.gz from URL\n",
      "DEBUG:src.logging:train-images-idx3-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring train-labels-idx1-ubyte.gz from URL\n",
      "DEBUG:src.logging:train-labels-idx1-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring t10k-images-idx3-ubyte.gz from URL\n",
      "DEBUG:src.logging:t10k-images-idx3-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring t10k-labels-idx1-ubyte.gz from URL\n",
      "DEBUG:src.logging:t10k-labels-idx1-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:f-mnist.license already exists and hash is valid\n",
      "DEBUG:src.logging:Creating f-mnist.readme from `contents` string\n",
      "DEBUG:src.logging:f-mnist.readme already exists and hash is valid\n",
      "INFO:src.logging:Ungzipping train-images-idx3-ubyte\n",
      "INFO:src.logging:Ungzipping train-labels-idx1-ubyte\n",
      "INFO:src.logging:Ungzipping t10k-images-idx3-ubyte\n",
      "INFO:src.logging:Ungzipping t10k-labels-idx1-ubyte\n",
      "INFO:src.logging:Copying f-mnist.license\n",
      "INFO:src.logging:Copying f-mnist.readme\n",
      "DEBUG:src.logging:Found cached Dataset for f-mnist: 1bdd754d481a6fe186e958508000a620555c61b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:(10000, 784), Target:(10000,)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"f-mnist\", kind=\"test\")\n",
    "print(f\"Data:{ds.data.shape}, Target:{ds.target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.logging:No file_name specified. Inferring train-images-idx3-ubyte.gz from URL\n",
      "DEBUG:src.logging:train-images-idx3-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring train-labels-idx1-ubyte.gz from URL\n",
      "DEBUG:src.logging:train-labels-idx1-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring t10k-images-idx3-ubyte.gz from URL\n",
      "DEBUG:src.logging:t10k-images-idx3-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:No file_name specified. Inferring t10k-labels-idx1-ubyte.gz from URL\n",
      "DEBUG:src.logging:t10k-labels-idx1-ubyte.gz already exists and hash is valid\n",
      "DEBUG:src.logging:f-mnist.license already exists and hash is valid\n",
      "DEBUG:src.logging:Creating f-mnist.readme from `contents` string\n",
      "DEBUG:src.logging:f-mnist.readme already exists and hash is valid\n",
      "INFO:src.logging:Ungzipping train-images-idx3-ubyte\n",
      "INFO:src.logging:Ungzipping train-labels-idx1-ubyte\n",
      "INFO:src.logging:Ungzipping t10k-images-idx3-ubyte\n",
      "INFO:src.logging:Ungzipping t10k-labels-idx1-ubyte\n",
      "INFO:src.logging:Copying f-mnist.license\n",
      "INFO:src.logging:Copying f-mnist.readme\n",
      "DEBUG:src.logging:Found cached Dataset for f-mnist: 048a21f52d05f88e50d70c47740ae1cf057549d2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:(60000, 784), Target:(60000,)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"f-mnist\", kind=\"train\")\n",
    "print(f\"Data:{ds.data.shape}, Target:{ds.target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check in the new `datasets.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check it in to source code control\n",
    "* do a `make data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
