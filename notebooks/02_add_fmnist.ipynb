{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are developing in a module, it's really handy to have these lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see debug-level logging in the notebook. Here's the incantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding and processing the Fashion-MNIST (FMNIST) Dataset\n",
    "\n",
    "A raw dataset is really just a list of files (and some useful metadata) that is later processed into a usable dataset. From a data provenance perspective, the most important things to know about raw data are:\n",
    "* Raw data is **hash-verified**. This ensures that if something changes upstream, we know about that change.\n",
    "* Raw data is **read only**, and is used to generate a separate and reproducible **Dataset** object\n",
    "* Raw data is **not saved** in the source code repository. (in fact, the whole `data` directory is specifically excluded in our `.gitignore`). Instead, the recipe for obtaining and processing the raw data is saved. (a snapshot of the raw data can be synced with a large data repo, like an AWS bucket, if desired.)\n",
    "\n",
    "Our approach to building a usable dataset is:\n",
    "\n",
    "1. Assemble the raw data files. Generate (and record) hashes to ensure the validity of these files.\n",
    "3. Add LICENSE and DESCR (description) metadata to make the raw data usable for other people, and\n",
    "4. Write a function to process the raw data into a usable format (for us, a `Dataset` object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"f-mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original MNIST dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "The dataset is free to use under an MIT license.\n",
    "\n",
    "It can be found online at https://github.com/zalandoresearch/fashion-mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directories, `paths` and `pathlib`\n",
    "\n",
    "Recall from our `README.md` the locations of our data files\n",
    "\n",
    "* `data`\n",
    "    * Data directory. often symlinked to a filesystem with lots of space\n",
    "    * `data/raw` \n",
    "        * Raw (immutable) hash-verified downloads\n",
    "    * `data/interim` \n",
    "        * Extracted and interim data representations, such as caches\n",
    "    * `data/processed` \n",
    "        * The final, cleaned and processed data sets for modeling.\n",
    "\n",
    "However, we **do not want to hardcode these paths** in our scripts.  This is what our `src.paths` module is for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import raw_data_path, interim_data_path, processed_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick aside: Use `pathlib`! Read more here:https://realpython.com/python-pathlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes exploring from a notebook a bit easier\n",
    "from src.data.utils import list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{raw_data_path}\")\n",
    "list_dir(raw_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Check Hashes\n",
    "The next step is to fetch these files and check (or generate) their hashes. The object we use to house this information is a `RawDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the FMNIST GitHub documentation, we see that the raw data is distributed as a set of 4 files. Because Zalando are excellent data citizens, they have conveniently given us MD5 hashes that we can verify when we download this data.\n",
    "\n",
    "| Name  | Content | Examples | Size | Link | MD5 Checksum|\n",
    "| --- | --- |--- | --- |--- |--- |\n",
    "| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n",
    "| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n",
    "| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n",
    "| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the raw files  and their hashes\n",
    "data_site = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com'\n",
    "file_list = [\n",
    "    ('train-images-idx3-ubyte.gz','8d4fb7e6c68d591d4c3dfef9ec88bf0d'),\n",
    "    ('train-labels-idx1-ubyte.gz','25c81989df183df01b3e8a0aad5dffbe'),\n",
    "    ('t10k-images-idx3-ubyte.gz', 'bef4ecab320f06d8554ea6380940ec79'),\n",
    "    ('t10k-labels-idx1-ubyte.gz', 'bb300cfdad3c16e7a12a480ee83cd310'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = RawDataset(dataset_name)\n",
    "for file, hashval in file_list:\n",
    "    url = f\"{data_site}/{file}\"\n",
    "    fmnist.add_url(url=url, hash_type='md5', hash_value=hashval)\n",
    "# Download and check the hashes\n",
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a License and Description\n",
    "How do we transform this data into a usable dataset? We need to know at least 2 more things:\n",
    "1. License: Am I allowed to use this data? Are there any restrictions on it's use, and\n",
    "2. Description: data about the raw data.\n",
    "\n",
    "Start with the **License**. Zalando are good data citizens, so their data License is directly available from\n",
    "their repo on github: https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice we tag this data with the name `LICENSE`\n",
    "fmnist.add_url(url='https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/LICENSE',\n",
    "            name='LICENSE', file_name=f'{dataset_name}.license')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we we should put together a quick **Description** of the dataset. Remember, you're writing this for *future you*, who will have long since forgotten what this is and where you got it. Include:  \n",
    "* What does the raw data look like?\n",
    "* Where did I get it from? \n",
    "* What format is it in?\n",
    "* What should it look like when it's processed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_readme = '''\n",
    "Fashion-MNIST\n",
    "=============\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 70000\n",
    "    :Number of Attributes: 728\n",
    "    :Attribute Information: 28x28 8-bit greyscale image\n",
    "    :Missing Attribute Values: None\n",
    "    :Creator: Zalando\n",
    "    :Date: 2017\n",
    "\n",
    "This is a copy of Zalando's Fashion-MNIST [F-MNIST] dataset:\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original [MNIST] dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "References\n",
    "----------\n",
    "  - [F-MNIST] Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\n",
    "    Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv:1708.07747\n",
    "  - [MNIST] The MNIST Database of handwritten digits. Yann LeCun, Corinna Cortes,\n",
    "    Christopher J.C. Burges. http://yann.lecun.com/exdb/mnist/\n",
    "'''\n",
    "\n",
    "fmnist.add_metadata(kind=\"DESCR\", contents=fmnist_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we modified the file list (adding the 2 metadata files), \n",
    "# fetch() will re-check and redownload all files if necessary\n",
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack the downloaded files\n",
    "By default, everything is unpacked to `interim_data_path/dataset_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_path = fmnist.unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unpack_path)\n",
    "list_dir(unpack_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the `RawDataset` into a `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fmnist.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fmnist.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{ds.DESCR}\\n\\n{ds.LICENSE}\\n\\n{ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.load_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a processing function\n",
    "We need to transform the raw data files into usable `data` and `target` vectors.\n",
    "The code at https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py tells us how to do that:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "unpack_path = fmnist.unpack()\n",
    "kind = \"train\"\n",
    "\n",
    "label_path = unpack_path / f\"{kind}-labels-idx1-ubyte\"\n",
    "with open(label_path, 'rb') as fd:\n",
    "    target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "dataset_path = unpack_path / f\"{kind}-images-idx3-ubyte\"\n",
    "with open(dataset_path, 'rb') as fd:\n",
    "    data = np.frombuffer(fd.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)\n",
    "\n",
    "print(f'Data: {data.shape}, Target: {target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset Constructor\n",
    "A processing function must produce a dictionary of kwargs that can be used in a call to `Dataset()`:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset\n",
    "help(Dataset.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A processing function **must** accept `dataset_name` and `metadata` as keywords. `dataset_name` should be obvious. `metadata` contains a dictionary that we will (optionally) add to as we construct the dataset.\n",
    "\n",
    "By convention we will store these functions in `src/data/localdata.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../src/data/localdata.py\n",
    "__all__ = ['process_mnist']\n",
    "\n",
    "from ..paths import interim_data_path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def process_mnist(dataset_name='mnist', kind='train', metadata=None):\n",
    "    '''\n",
    "    Load the MNIST dataset (or a compatible variant; e.g. F-MNIST)\n",
    "\n",
    "    dataset_name: {'mnist', 'f-mnist'}\n",
    "        Which variant to load\n",
    "    kind: {'train', 'test'}\n",
    "        Dataset comes pre-split into training and test data.\n",
    "        Indicates which dataset to load\n",
    "    metadata: dict\n",
    "        Additional metadata fields will be added to this dict.\n",
    "        'kind': value of `kind` used to generate a subset of the data\n",
    "    '''\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "        \n",
    "    if kind == 'test':\n",
    "        kind = 't10k'\n",
    "\n",
    "    label_path = interim_data_path / dataset_name / f\"{kind}-labels-idx1-ubyte\"\n",
    "    with open(label_path, 'rb') as fd:\n",
    "        target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "    dataset_path = interim_data_path / dataset_name / f\"{kind}-images-idx3-ubyte\"\n",
    "    with open(dataset_path, 'rb') as fd:\n",
    "        data = np.frombuffer(fd.read(), dtype=np.uint8,\n",
    "                                       offset=16).reshape(len(target), 784)\n",
    "    metadata['subset'] = kind\n",
    "    \n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "        'metadata': metadata,\n",
    "    }\n",
    "    return dset_opts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.localdata import process_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Partial functions\n",
    "We can build a new function from an existing function, fixing some of the parameters. In python, this is called a `partial`. (In other languages, you might hear it called a *closure*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "fmnist.load_function = partial(process_mnist, dataset_name='f-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try processing our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fmnist.process(force=True)\n",
    "ds.data.shape, ds.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To and From JSON\n",
    "We want to be able to save and load this Raw Dataset object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = fmnist.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = RawDataset.from_json('f-mnist', json_str=jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.fetch()\n",
    "f2.unpack()\n",
    "ds2 = f2.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.data.shape, ds2.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE BE DRAGONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a `Dataset` object?\n",
    "* data: the processed data\n",
    "* target: (optional) target vector (for supervised learning problems)\n",
    "* metadata: Data about the data\n",
    "\n",
    "Under the hood, this is basically a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, this is empty, since we haven't given any indication how to process the raw files into usable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the raw files into `data` and `target`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we turn these raw files into processed data?\n",
    "\n",
    "First, we need to unpack them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import fetch_and_unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untar_dir = fetch_and_unpack(dataset_name)\n",
    "print(f\"{untar_dir}:\\n {list_dir(untar_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch_and_unpack knows how to handle most compressed data types. Unpacked data is stored at `interim_data_path/dataset_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(interim_data_path / dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
    "we see how to process this data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need numpy. How do we add this to the environment?\n",
    "* Add it to `environment.yml`\n",
    "* `make requirements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = \"train\"\n",
    "label_path = interim_data_path / dataset_name / f\"{kind}-labels-idx1-ubyte\"\n",
    "with open(label_path, 'rb') as fd:\n",
    "    target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "dataset_path = interim_data_path / dataset_name / f\"{kind}-images-idx3-ubyte\"\n",
    "with open(dataset_path, 'rb') as fd:\n",
    "    data = np.frombuffer(fd.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is this data? We should document what this Dataset represents. Enter the first of two special pieces of metadata: DESCR. Let's be nice to our users and document out dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_readme = '''\n",
    "Fashion-MNIST\n",
    "=============\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 70000\n",
    "    :Number of Attributes: 728\n",
    "    :Attribute Information: 28x28 8-bit greyscale image\n",
    "    :Missing Attribute Values: None\n",
    "    :Creator: Zalando\n",
    "    :Date: 2017\n",
    "\n",
    "This is a copy of Zalando's Fashion-MNIST [F-MNIST] dataset:\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original MNIST dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "References\n",
    "----------\n",
    "  - [F-MNIST] Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\n",
    "    Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv:1708.07747\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import add_dataset_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dataset_metadata(dataset_name, kind='DESCR', from_str=fmnist_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyc_easydata]",
   "language": "python",
   "name": "conda-env-nyc_easydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
