{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning a `DataSource` into a `Dataset`\n",
    "How do we turn raw data into something useful? There are 2 steps:\n",
    "1. Write a function to extract meaningful `data` (and optionally, `target`) objects from your raw files, and\n",
    "2. Wrap this function in the form of a **processing function**\n",
    "\n",
    "\n",
    "First, let's grab the dataset we created in the last notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading DataSets from the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow\n",
    "from src.data import DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource.from_name('lvq-pak')    # load it from the catalog\n",
    "unpack_dir = dsrc.unpack()                # Find the location of the unpacked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $unpack_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Function Template\n",
    "A processing function is a function that \n",
    "* takes at least 2 keyword arguments as input: `dataset_name` (a string) and `metadata` (a dict).\n",
    "* Returns a dictionary with the following keys: `dataset_name`, `data`, `target` (optional), and `metadata`\n",
    "Here's a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(dataset_name='raw_data', metadata=None):\n",
    "    \"\"\"Process a raw dataset object\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    extract_func: function returning tuple: (data, target)\n",
    "        Function to extract data and target\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    data, target = None, None\n",
    "\n",
    "    # Generate `data` and `target` info\n",
    "    #    data, target = extract_func()\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Processing lvq-pak data\n",
    "Bj√∏rn has successfully fetched and extracted the lvq-pak data. Now he is ready to process it into `data` and `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource.from_name('lvq-pak')    # load it from the catalog\n",
    "unpack_dir = dsrc.unpack()                # Find the location of the unpacked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(unpack_dir) # what's the extracted data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(unpack_dir / 'lvq_pak-3.1')  # Files are extracted to a subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_train = unpack_dir / 'lvq_pak-3.1' / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'lvq_pak-3.1' / 'ex2.dat'\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import head_file\n",
    "print(head_file(datafile_train)) # number of data columns, followed by comment, then space-delimited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(head_file(datafile_test)) # similar, but no comment header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True):\n",
    "    \"\"\"Read an space-delimited file\n",
    "    \n",
    "    Data is space-delimited. Last column is the (string) label for the data\n",
    "\n",
    "    Note: we can't use automatic comment detection, as `#` characters are also used as data labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skiprows: None or list\n",
    "        list of rows to skip when reading the file.\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class (target) label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_table(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lvq_pak(dataset_name='lvq-pak', metadata=None, kind='all'):\n",
    "    \"\"\"Process LVQ-data object\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    extract_func: function returning tuple: (data, target)\n",
    "        Function to extract data and target\n",
    "    kind: {'train', 'test', 'all'}\n",
    "        Whether to return training set, test set, or everything. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    untar_dir = interim_data_path / dataset_name\n",
    "    unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "    elif kind == 'test':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "    elif kind == 'all':\n",
    "        data1, target1 = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "        data2, target2 = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_lvq_pak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.load_function = process_lvq_pak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsrc.process() # Use the load_function to convert this DataSource to a real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Built Dataset: {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsrc.process(kind=\"test\")  # Should be half the size\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE: Process Mark's F-MNIST Data\n",
    "In the last exercise, you fetched and unpacked F-MNIST data.\n",
    "Now it's time to process it into a usable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Dataset` and Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tour of the Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complicated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Data: The Punchline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
