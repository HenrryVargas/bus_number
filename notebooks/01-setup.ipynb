{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cookiecutter cookiecutter-easydata\n",
    "```\n",
    "fill it in\n",
    "\n",
    "```\n",
    "cd nyc_easydata\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "explore README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nyc_easydata\n",
    "==============================\n",
    "\n",
    "Reproducible Data Science Tutorial\n",
    "\n",
    "GETTING STARTED\n",
    "---------------\n",
    "\n",
    "* Create and switch to the  virtual environment:\n",
    "```\n",
    "make create environment\n",
    "conda activate nyc_easydata\n",
    "```\n",
    "* Fetch the raw data and process it into a usable form\n",
    "```\n",
    "make data\n",
    "```\n",
    "* Explore the notebooks in the `notebooks` directory\n",
    "\n",
    "Project Organization\n",
    "------------\n",
    "* `LICENSE`\n",
    "* `Makefile`\n",
    "    * top-level makefile. Type `make` for a list of valid commands\n",
    "* `README.md`\n",
    "    * this file\n",
    "* `data`\n",
    "    * Data directory. often symlinked to a filesystem with lots of space\n",
    "    * `data/raw`\n",
    "        * Raw (immutable) hash-verified downloads\n",
    "    * `data/interim`\n",
    "        * Extracted and interim data representations\n",
    "    * `data/processed`\n",
    "        * The final, canonical data sets for modeling.\n",
    "* `docs`\n",
    "    * A default Sphinx project; see sphinx-doc.org for details\n",
    "* `models`\n",
    "    * Trained and serialized models, model predictions, or model summaries\n",
    "* `notebooks`\n",
    "    *  Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "    the creator's initials, and a short `-` delimited description,\n",
    "    e.g. `1.0-jqp-initial-data-exploration`.\n",
    "* `references`\n",
    "    * Data dictionaries, manuals, and all other explanatory materials.\n",
    "* `reports`\n",
    "    * Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "    * `reports/figures`\n",
    "        * Generated graphics and figures to be used in reporting\n",
    "* `requirements.txt`\n",
    "    * (if using pip+virtualenv) The requirements file for reproducing the\n",
    "    analysis environment, e.g. generated with `pip freeze > requirements.txt`\n",
    "* `environment.yml`\n",
    "    * (if using conda) The YAML file for reproducing the analysis environment\n",
    "* `setup.py`\n",
    "    * Turns contents of `src` into a\n",
    "    pip-installable python module  (`pip install -e .`) so it can be\n",
    "    imported in python code\n",
    "* `src`\n",
    "    * Source code for use in this project.\n",
    "    * `src/__init__.py`\n",
    "        * Makes src a Python module\n",
    "    * `src/data`\n",
    "        * Scripts to fetch or generate data. In particular:\n",
    "        * `src/data/make_dataset.py`\n",
    "            * Run with `python -m src.data.make_dataset fetch`\n",
    "            or  `python -m src.data.make_dataset process`\n",
    "    * `src/features`\n",
    "        * Scripts to turn raw data into features for modeling, notably `build_features.py`\n",
    "    * `src/models`\n",
    "        * Scripts to train models and then use trained models to make predictions.\n",
    "        e.g. `predict_model.py`, `train_model.py`\n",
    "    * `src/visualization`\n",
    "        * Scripts to create exploratory and results oriented visualizations; e.g.\n",
    "        `visualize.py`\n",
    "* `tox.ini`\n",
    "    * tox file with settings for running tox; see tox.testrun.org\n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "<p><small>Project derived from the the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>, for experimenting with ideas to improve the template  #cookiecutterdatascience</small></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next section can come from following the README.md\n",
    "```\n",
    "make create_environment\n",
    "source activate nyc_easydata\n",
    "```\n",
    "\n",
    "```\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"initial import from cookiecutter-easydata\"\n",
    "```\n",
    "(add github commands to create/sync to this repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Slide initial intro to the overall workflow\n",
    "\n",
    "Datascience as a DAG: the Makefile\n",
    "\n",
    "Here's the flow:\n",
    "* `make data`\n",
    "* `make train`\n",
    "* `make predict`\n",
    "* `make analysis`\n",
    "* `make summarize`\n",
    "* `make publish`\n",
    "\n",
    "The easydata cookiecutter has a built in framework, but **you have to fill in the scripts!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data\n",
    "\n",
    "From README.md\n",
    "\n",
    "* `data`\n",
    "    * Data directory. often symlinked to a filesystem with lots of space\n",
    "    * `data/raw`\n",
    "        * Raw (immutable) hash-verified downloads\n",
    "    * `data/interim`\n",
    "        * Extracted and interim data representations\n",
    "    * `data/processed`\n",
    "        * The final, canonical data sets for modeling.\n",
    "\n",
    "From the Makefile\n",
    "```\n",
    "## Perform the complete dataset generation pipeline\n",
    "data: requirements process_data\n",
    "\n",
    "## Fetch the data\n",
    "fetch_data:\n",
    "\t$(PYTHON_INTERPRETER) -m src.data.make_dataset fetch\n",
    "\n",
    "## Fetch and process the data\n",
    "process_data: fetch_data\n",
    "\t$(PYTHON_INTERPRETER) -m src.data.make_dataset process\n",
    "    \n",
    "## Install or update Python Dependencies\n",
    "requirements: test_environment\n",
    "\tconda env update --name $(PROJECT_NAME) -f environment.yml\n",
    "\n",
    "## Test python environment is set-up correctly\n",
    "test_environment:\n",
    "ifneq (${CONDA_DEFAULT_ENV}, $(PROJECT_NAME))\n",
    "\t$(error Must activate `$(PROJECT_NAME)` environment before proceeding)\n",
    "endif\n",
    "\t$(PYTHON_INTERPRETER) test_environment.py\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset object\n",
    "\n",
    "Gets written to `src/data/dataset.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Example of F-MNIST data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Follow lvq-pak data setup from the other notebook\n",
    "http://www.cis.hut.fi/research/lvq_pak/\n",
    "\n",
    "README: http://www.cis.hut.fi/research/lvq_pak/README\n",
    "\n",
    "Text: http://www.cis.hut.fi/research/lvq_pak/lvq_doc.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bus_number]",
   "language": "python",
   "name": "conda-env-bus_number-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
