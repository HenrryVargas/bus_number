{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bjørn's Problem: Supervised Learning\n",
    "\n",
    "Bjørn employs a large number of Finnish line cooks. He can’t understand a word they say.\n",
    "\n",
    "Bjørn needs a trained model to do real-time translation from Finnish to Swedish.\n",
    "\n",
    "Bjørn has decided to start with the Finnish phoneme dataset shipped with a project called lvq-pak. His objective is to train three different models, and choose the one with the best overall accuracy score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are developing in a module, it's really handy to have these lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see debug-level logging in the notebook. Here's the incantation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data is Read Only: Sing it with me!\n",
    "\n",
    "How do you asseble raw data into a usable dataset? How do you make sure your raw datasets are reproducible?\n",
    "We are going to work through the process of collecting raw data files (using an object called a `RawDataset`), and converting them into something useful for our reproducible data science workflow (the `Dataset` object).\n",
    "\n",
    "A raw dataset is really just a list of files (and some useful metadata) that is later processed into a usable dataset. From a data provenance perspective, the most important things to know about raw data are:\n",
    "* Raw data is **hash-verified**. This ensures that if something changes upstream, we know about that change.\n",
    "* Raw data is **read only**, and is used to generate a separate and reproducible **Dataset** object\n",
    "* Raw data is **not saved** in the source code repository. (in fact, the whole `data` directory is specifically excluded in our `.gitignore`). Instead, the recipe for obtaining and processing the raw data is saved. A snapshot of the raw data can be synced with a large data repo, like an AWS bucket, if desired.\n",
    "\n",
    "Our approach to building a usable dataset is:\n",
    "\n",
    "1. Assemble the raw data files. Generate (and record) hashes to ensure the validity of these files.\n",
    "2. Add LICENSE and DESCR (description) metadata to make the raw data usable for other people, and\n",
    "3. Write a function to process the raw data into a usable format (for us, a `Dataset` object)\n",
    "4. Write transformation functions on `Dataset` objects that fit our data munging into an automated reproducible workflow\n",
    "\n",
    "Let's build our first `Dataset`.\n",
    "\n",
    "\n",
    "### LVQ-Pak: A Finnish Phonetic dataset\n",
    "\n",
    "The Learning Vector Quantization (lvq-pak) project includes a simple Finnish phonetic dataset\n",
    "consisting 20-dimensional Mel Frequency Cepstrum Coefficients (MFCCs) labelled with target phoneme information. Our goal is to explore this dataset, process it into a useful form, and make it a part of a reproducible data science workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='lvq-pak'     # Naming things: the hardest problem in computer science."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the Raw Data Files\n",
    "lvq-pak is shipped as a source code tarball. The data files are included as textfiles within that download. Our first goal is to retrieve these raw source files, and record some metadata about them, such as hashes that uniquely identify them.\n",
    "\n",
    "<img src=\"../references/workflow/make-raw.png\" alt=\"make raw step\" width=500/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But first...Data Directories, `paths` and `pathlib`\n",
    "\n",
    "Recall from our `README.md` the locations of our data files\n",
    "\n",
    "* `data`\n",
    "    * Data directory. often symlinked to a filesystem with lots of space\n",
    "    * `data/raw` \n",
    "        * Raw (immutable) hash-verified downloads\n",
    "    * `data/interim` \n",
    "        * Extracted and interim data representations, such as caches\n",
    "    * `data/processed` \n",
    "        * The final, cleaned and processed data sets for modeling.\n",
    "\n",
    "However, we **do not want to hardcode these paths** in our scripts.  This is what our `src.paths` module is for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import raw_data_path, interim_data_path, processed_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASIDE: Use `pathlib`\n",
    "We will be using `pathlib` under the hood so that our paths are platform independant. They are also **much** easier to work with than old-fashioned strings.\n",
    "\n",
    "**Use `Pathlib`!** \n",
    "\n",
    "Read more here:https://realpython.com/python-pathlib/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what's in these directories already...they should, at this stage, be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import list_dir\n",
    "# silly helper function, but makes exploring from a notebook a bit easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{raw_data_path}\")\n",
    "list_dir(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Check Hashes\n",
    "Let's fetch the source files for lvq-pak and check (or generate) their hashes. The object we use to house this information is a `RawDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can add a file to the RawDataset using its `add_url()` method. If we know the hash of this file, we should include it here, and it will be checked on download. If not, one will be computed from this download and used for comparison on subsequent downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak = RawDataset(dataset_name)\n",
    "lvq_pak.add_url(url=\"http://www.cis.hut.fi/research/lvq_pak/lvq_pak-3.1.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add License and Description\n",
    "Before we can turn this raw data into a usable dataset, we need to know two things:\n",
    "1. What does the raw data look like? Where did I get it from? What format is it in? What should it look like when it's processed? (`DESCR`)\n",
    "2. Am I allowed to use this data? (`LICENSE`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some additional options to `add_url()` we may wish to use for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(RawDataset.add_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `hash_value` and `hash_type` should be self-evident.\n",
    "* `file_name` is optional. If skipped, the name will be guessed from the URL. It's often nice to specify, however, so you can use this attribute to override the default guess.\n",
    "* The `name` field can either be used to specify a text label to describe the dataset. It can also be used to tag two special pieces of metadata: `DESCR` and `LICENSE`, the downloaded (text) file will be used as the dataset description and license text, respectively.\n",
    "\n",
    "We will use this feature to add a `README` to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak.add_url(url='http://www.cis.hut.fi/research/lvq_pak/README',\n",
    "                file_name=f'{dataset_name}.readme',\n",
    "                name='DESCR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets should ***always* have an explicit license**. Reading the project documentation, we see a license in one of the textfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "license_txt = '''\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*                              LVQ_PAK                                 *\n",
    "*                                                                      *\n",
    "*                                The                                   *\n",
    "*                                                                      *\n",
    "*                   Learning  Vector  Quantization                     *\n",
    "*                                                                      *\n",
    "*                          Program  Package                            *\n",
    "*                                                                      *\n",
    "*                   Version 3.1 (April 7, 1995)                        *\n",
    "*                                                                      *\n",
    "*                          Prepared by the                             *\n",
    "*                    LVQ Programming Team of the                       *\n",
    "*                 Helsinki University of Technology                    *\n",
    "*           Laboratory of Computer and Information Science             *\n",
    "*                Rakentajanaukio 2 C, SF-02150 Espoo                   *\n",
    "*                              FINLAND                                 *\n",
    "*                                                                      *\n",
    "*                      Copyright (c) 1991-1995                         *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*  NOTE: This program package is copyrighted in the sense that it      *\n",
    "*  may be used for scientific purposes. The package as a whole, or     *\n",
    "*  parts thereof, cannot be included or used in any commercial         *\n",
    "*  application without written permission granted by its producents.   *\n",
    "*  No programs contained in this package may be copied for commercial  *\n",
    "*  distribution.                                                       *\n",
    "*                                                                      *\n",
    "*  All comments concerning this program package may be sent to the     *\n",
    "*  e-mail address 'lvq@nucleus.hut.fi'.                                *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract and use the license via the `add_metadata()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak.add_metadata(contents=license_txt, kind='LICENSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the Data\n",
    "\n",
    "We're ready to download the data. Now that we've set up our `RawDataset`, all we have to do now is `fetch()` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import list_dir\n",
    "from src.paths import raw_data_path\n",
    "list_dir(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack the data\n",
    "How do we turn these raw files into processed data?\n",
    "\n",
    "First, we need to unpack them. Fortunately, the `RawDataset` object knows how to unpack or decompress most common archive formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untar_dir = lvq_pak.unpack()\n",
    "print(untar_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, raw data is unpacked to `interim_data_path` in a directory corresponding to the `dataset_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(interim_data_path / dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a `Dataset` object\n",
    "\n",
    "A `Dataset` is the fundamental object we use for making the \"munge\" part of our data flow reproducible\n",
    "\n",
    "What's a `Dataset` object?\n",
    "It's a scikit-learn-style `Bunch`, containing:\n",
    "* `data`: the processed data\n",
    "* `target`: (optional) target vector (for supervised learning problems)\n",
    "* `metadata`: Data about the data\n",
    "\n",
    "Under the hood, this is esentially a dictionary, with a little bit of magic to make it nicer to work with. \n",
    "\n",
    "### Converting a `RawDataset` into a usable `Dataset`\n",
    "\n",
    "`RawDataset` objects have a `process()` method that emit a `Dataset`.\n",
    "\n",
    "Suppose we do that right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = lvq_pak.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Dataset` has metdata: (e.g. a license):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here's one of the ways a `Dataset` makes your life a little easier. If you specify an attribute name in upper case, the dataset assumes it is metadata, and looks it in the metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.metadata['license'] == ds.LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` doesn't actually have any data yet. In particular, `data` and `target` are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../references/workflow/make-data.png\" alt=\"make data step\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to write the importer that actually processes the data we will be using for this dataset.\n",
    "\n",
    "The important things to generate are `data` and `target` entries. A `metadata` is optional, but recommended if you want to save additional information about the dataset.\n",
    "\n",
    "Usually, this functionality gets bundled up into a function and added to `datasets.py`\n",
    "\n",
    "Let's find our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "list_dir(unpack_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the training and test datsets are stored in files named `ex1.dat` and `ex2.dat` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the file\n",
    "untar_dir = lvq_pak.unpack()\n",
    "list_dir(untar_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_train = unpack_dir / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'ex2.dat'\n",
    "\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, the data format is space-delimited, with the class label included as the last column. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import head_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(head_file(datafile_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the datafile consists of a single line containing the dimension of the data, a comment, and then 21 space-delimited columns, the final column being the target class label. \n",
    "\n",
    "**Note:** We have to be a little careful importing the data, because '#' is used both as the comment delimiter, and as a class label.\n",
    "\n",
    "Fortunately, we have a helper function for this. We will get a little cheeky and skip the first 2 lines (hoping there are no other comments). The documentation also says ther are 1962 entries in each of the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True):\n",
    "    \"\"\"Read an space-delimited file\n",
    "\n",
    "    skiprows: list of rows to skip when reading the file.\n",
    "\n",
    "    Note: we can't use automatic comment detection, as\n",
    "    `#` characters are also used as data labels.\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_table(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data2, target2 = read_space_delimited(datafile_test, skiprows=[0])\n",
    "\n",
    "data.shape, target.shape, data2.shape, target2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work, so let's wrap this functionality up into a processing function.\n",
    "By convention, a processing function takes a `dataset_name`, and any other options that may be useful for reading the data, and returns a dictionary that matches the `Dataset` constructor signature.\n",
    "\n",
    "In addition, a processing function should accept `dataset_name` and `metadata` keywords. Any additional metadata should be added to the `metadata` object that is passed in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset\n",
    "help(Dataset.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will place this function in `localdata.py`, (and add it to `__all__`) to make it visible to our dataset code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file ../src/data/localdata.py\n",
    "\"\"\"\n",
    "Custom dataset processing/generation functions should be added to this file\n",
    "\"\"\"\n",
    "\n",
    "from src.data.utils import read_space_delimited, normalize_labels\n",
    "from src.paths import interim_data_path\n",
    "import numpy as np\n",
    "\n",
    "__all__ = ['process_lvq_pak']\n",
    "\n",
    "def process_lvq_pak(dataset_name='lvq-pak', kind='all', numeric_labels=True, metadata=None):\n",
    "    \"\"\"\n",
    "    kind: {'test', 'train', 'all'}, default 'all'\n",
    "    numeric_labels: boolean (default: True)\n",
    "        if set, target is a vector of integers, and label_map is created in the metadata\n",
    "        to reflect the mapping to the string targets\n",
    "    \"\"\"\n",
    "    \n",
    "    untar_dir = interim_data_path / dataset_name\n",
    "    unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "    elif kind == 'test':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "    elif kind == 'all':\n",
    "        data1, target1 = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "        data2, target2 = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    if numeric_labels:\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        mapped_target, label_map = normalize_labels(target)\n",
    "        metadata['label_map'] = label_map\n",
    "        target = mapped_target\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "    return dset_opts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset\n",
    "from src.data.localdata import process_lvq_pak\n",
    "\n",
    "for kind in ['train', 'test', 'all']:\n",
    "    dset_opts = process_lvq_pak(kind=kind)\n",
    "    dset = Dataset(**dset_opts)\n",
    "    print(f'{kind}: data={dset.data.shape} target={dset.target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all looks good. We can add this process function to the built in workflow in order to automate `Dataset` creation.\n",
    "\n",
    "## Adding this Dataset to the master dataset list\n",
    "All the commands for manipulating the various lists of activities in our data science project are in a module called `workflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, add this **processing function** to the `RawDataset`, then add the `RawDataset` to the global dataset list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak.load_function = process_lvq_pak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_raw_dataset(lvq_pak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, re-load the dataset and save a copy of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq = Dataset.from_raw(dataset_name, force=True)\n",
    "print(str(lvq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, we can create an empty `transformer` to convert this RawDataset into a Dataset and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_transformer(from_raw=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_transformer_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running `apply_transforms()` a complete dataset will be written to `processed_data_path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "workflow.make_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy of just the metadata is also stored, so that it may be quickly checked without loading the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_meta = Dataset.load(dataset_name, metadata_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lvq_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lvq.DATA_HASH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even better, from now on, **anytime** we want to access this dataset, all we have to do now is load the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_from_file = Dataset.load(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_from_file.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you didn't realize you were automating\n",
    "\n",
    "By adding `lvq-pak` and it's processing functions to the `workflow`, you've now completely automated the process of fetching, unpacking, processing and transforming your data. In fact, there are a bunch of `Makefile` targets that will now work.\n",
    "\n",
    "\n",
    "    fetch_raw:\n",
    "        $(PYTHON_INTERPRETER) -m src.data.make_dataset fetch\n",
    "\n",
    "    unpack_raw:\n",
    "        $(PYTHON_INTERPRETER) -m src.data.make_dataset unpack\n",
    "\n",
    "    process_raw:\n",
    "        $(PYTHON_INTERPRETER) -m src.data.make_dataset process\n",
    "\n",
    "    ## Apply Transformations to produce fully processed Datsets\n",
    "    transform_data:\n",
    "        $(PYTHON_INTERPRETER) -m src.data.apply_transforms transformer_list.json\n",
    "\n",
    "The main difference here, is that the make targets will run on everything (all different datasets) that you've added to your workflow. We'll see this in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make fetch_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make unpack_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make process_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above three make targets are wrapped up in a single target:\n",
    "\n",
    "    ## Fetch, Unpack, and Process raw dataset files\n",
    "    raw: requirements process_raw\n",
    "    \n",
    "**Note**: the `-n` option to `make` doesn't run any commands, just prints them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make -n raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the exact same process by calling a workflow function, cleverly named `make_raw()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.make_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't all we did, we also transformed our data into a `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to run **everything** that we've done so far, all you need is:\n",
    "\n",
    "    ## convert raw datasets into fully processed datasets\n",
    "    data: raw transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make -n data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or equivalently\n",
    "workflow.make_raw()\n",
    "workflow.make_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASIDE: The magical `src` package\n",
    "You might notice that most of our code is imported from a module called `src`. \n",
    "E.g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you're wondering what this is all about? The magic piece that makes this module work is in `enviroment.yml`, our conda environment specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import paths\n",
    "from src.utils import list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `- -e .` line. This installs an editable version of the module described in `setup.py`. This is where we keep all the interesting source code associated with our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths.src_module_dir.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Transformations to Datasets\n",
    "Once we build a basic Dataset out of its raw data components, it's very common to want to produce variants of this dataset through some kind of data transformation. Our reproducible workflow makes this easy; e.g.\n",
    "\n",
    "### Train/Test Split\n",
    "\n",
    "Since the tasks we have for Bjørn is to build a supervised model, we will undoubtably want to be able to create a train/test split. The default we had from downloading was a 50/50 split. But what if we actually want an 80/20 split?\n",
    "\n",
    "Let's script this into the Dataset creation process.\n",
    "\n",
    "We can  add a **transformer** that builds the train/test split. Since this is a transformation we want to do all the time, we already have one built in to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workflow.available_transformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(workflow.available_transformers(keys_only=False)['train_test_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_transformer_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add this transformer to re-split the `lvq-pak` data and add a new `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = [(\"train_test_split\", {'random_state':6502, 'test_size':0.2})]\n",
    "workflow.add_transformer(from_raw='lvq-pak',\n",
    "                         suppress_output=True,\n",
    "                         transformations=transform_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO) # This step is a bit noisy, otherwise.\n",
    "workflow.make_data()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there are now two new datasets: `lvq-pak_test` and `lvq-pak_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets can now be loaded the way that `lvq-pak` could already be loaded. Furthermore, the metadata now reflects the options that were used to generate this split in a metadata field called `split_opts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('lvq-pak_train')\n",
    "ds.SPLIT_OPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, any other munging that we want to do to our data can be done via a **transformer**. That way, our RawDataset stays **read-only**, and all of our munging is scripted and reproducible, prodicing named Dataset objects. We can also easily access these transformed data (via `Dataset.load()`)whenever we need them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here are the steps Bjørn took to automate the fetching and munging of his Finnish phoneme data:\n",
    "* He created a `RawDataset` that knew about file locations and hashes for the raw data.\n",
    "* He added metadata documenting the format (DESCR) and legal info (LICENSE) associated with this data.\n",
    "* He created a basic `Dataset` from this `RawDataset`\n",
    "* He build a tranformer to perform an 80/20 train/test split of his data\n",
    "\n",
    "Here's the whole process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset, RawDataset\n",
    "from src.data.localdata import process_lvq_pak\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Define and Add a raw dataset\n",
    "raw_ds = RawDataset('lvq-pak')\n",
    "raw_ds.add_url(url=\"http://www.cis.hut.fi/research/lvq_pak/lvq_pak-3.1.tar\")\n",
    "raw_ds.add_url(url='http://www.cis.hut.fi/research/lvq_pak/README',\n",
    "                file_name=f'{dataset_name}.readme',\n",
    "                name='DESCR')\n",
    "raw_ds.add_metadata(contents=license_txt, kind='LICENSE')\n",
    "raw_ds.load_function = process_lvq_pak\n",
    "workflow.add_raw_dataset(raw_ds)\n",
    "workflow.make_raw()\n",
    "\n",
    "# Convert the raw dataset into a Dataset, and add a 80/20 train/test split \n",
    "workflow.add_transformer(from_raw='lvq-pak')\n",
    "transform_pipeline = [(\"train_test_split\", {'random_state':6502, 'test_size':0.2})]\n",
    "workflow.add_transformer(from_raw='lvq-pak',\n",
    "                         suppress_output=True,\n",
    "                         transformations=transform_pipeline)\n",
    "workflow.make_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bjørn now `Dataset.load()` his `lvq-pak_train`, `lvq-pak_test`, and `lvq-pak` datsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASIDE: Using `git` to record these changes\n",
    "Under the hood, the all the elements to the `workflow` are being saved as JSON files. In particular:\n",
    "* `make raw` produces a `raw_datasets.json`\n",
    "* `make data` produces a `transformer_list.json`\n",
    "* We added code to `src/data/localdata.py`\n",
    "\n",
    "Because you're using `git` to keep track of your workflow, it's now time to check all these changes in.\n",
    "\n",
    "```git status```\n",
    "\n",
    "That means checking in the `.json` files that keep track of what your data-related `make` targets are going to do.\n",
    "\n",
    "Cheat sheet:\n",
    "\n",
    "    git add -p src/data/raw_datasets.json\n",
    "    git add -p src/data/transformer_list.json\n",
    "    git add -p src/data/localdata.py\n",
    "    git commit -m \"(your commit message)\"\n",
    "    \n",
    "Your project now generates its usable datasets from raw files. And it's all checked into source control and automated using `Makefile` targets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Reproducible Data Science!##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bus_number]",
   "language": "python",
   "name": "conda-env-bus_number-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
