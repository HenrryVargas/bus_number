{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Reproducible Data (1h)\n",
    "*\"Raw Data is Read Only. Sing it with me\"*\n",
    "\n",
    "\n",
    "* RawDataset\n",
    "  * Fetching + Unpack\n",
    "      * Example 1: lvq-pak\n",
    "      * Exercise: fmnist\n",
    "  * Processing data\n",
    "    * Process into data, (optionally, target)\n",
    "    * create a process_my_dataset() function\n",
    "        * Example 1: lvq-pak\n",
    "        * Exercise: fmnist\n",
    "    * save the raw dataset to the raw dataset catalog\n",
    "\n",
    "* Datasets and Data Transformers\n",
    "    * Create a transformer to produce a Dataset from the RawDataset\n",
    "    * Add this dataset to the catalog\n",
    "    * Load the dataset\n",
    "        * example: lvq-pak\n",
    "        * exercise: fmnist_test, fmnist_train\n",
    "    \n",
    "    * More Complicated Transformers\n",
    "        * Example: Train/Test Split on lvq-pak\n",
    "        * Exercise: merge labels on lvq-pak\n",
    "        * Exercise: merge labels on fmnist\n",
    "        \n",
    "* Punchline: \n",
    "  * make clean_raw, clean_cache, clean_processed, (clean_data?) `make data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bjørn\n",
    "* Bjørn is a dot-com millionaire. Currently he heads the Ikea R&D kitchen in Sweden.\n",
    "* Bjørn employs a large number of Finnish line cooks. He can’t understand a word they say.\n",
    "* Bjørn needs a **trained model** to do real-time translation from Finnish to Swedish.\n",
    "* Reproducibility means that even though test kitchens have notoriously high turnover, Bjørn can ask his sous-chef to **deploy updated models** whenever needed.\n",
    "\n",
    "### Mark\n",
    "* Mark used to be an astronaut. After a rough year, he decided to change careers, and now works for a fashion magazine in NYC.\n",
    "* Mark has to keep up with thousands of new fashions every week. More if it’s Fashion Week.\n",
    "* Mark wants to **publish a paper and software library** for accelerating fashion review using science. \n",
    "* Reproducibility means Mark's peer reviewers can **reproduce his work**, his paper will get accepted, his library will get pulled into *scikit-learn*, and he will become a hero to the data-based fashion industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `RawDataset`\n",
    "The `RawDataset` object handles downloading, unpacking, and processing raw data files, and serves as a container for some basic metadata, including **documentation** and **license** information.\n",
    "\n",
    "\n",
    "\n",
    "Raw data files are downloaded to  `paths.raw_data_path`.\n",
    " Cache files and unpacked raw files are saved to `paths.interim_data_path`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and Unpacking Raw Data\n",
    "#### Example: Bjørn and lvq-pak\n",
    "\n",
    "##### LVQ-Pak: A Finnish Phonetic dataset\n",
    "\n",
    "The Learning Vector Quantization (lvq-pak) project includes a simple Finnish phonetic dataset\n",
    "consisting 20-dimensional Mel Frequency Cepstrum Coefficients (MFCCs) labelled with target phoneme information. Our goal is to explore this dataset, process it into a useful form, and make it a part of a reproducible data science workflow. The project can be found at: http://www.cis.hut.fi/research/lvq_pak/\n",
    "\n",
    "For this example, we are going to help Bjørn create a `RawDataset` by:\n",
    "1. Download and unpack the raw data files. \n",
    "2. Generate (and record) hash values for these files.\n",
    "2. Add relevant LICENSE and DESCR (description) metadata to this RawDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset\n",
    "from src.utils import list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = RawDataset('lvq-pak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds.add_url(\"http://www.cis.hut.fi/research/lvq_pak/lvq_pak-3.1.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds.add_url(\"http://www.cis.hut.fi/research/lvq_pak/README\", file_name='lvq-pak.readme', name='DESCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "license_txt = '''\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*                              LVQ_PAK                                 *\n",
    "*                                                                      *\n",
    "*                                The                                   *\n",
    "*                                                                      *\n",
    "*                   Learning  Vector  Quantization                     *\n",
    "*                                                                      *\n",
    "*                          Program  Package                            *\n",
    "*                                                                      *\n",
    "*                   Version 3.1 (April 7, 1995)                        *\n",
    "*                                                                      *\n",
    "*                          Prepared by the                             *\n",
    "*                    LVQ Programming Team of the                       *\n",
    "*                 Helsinki University of Technology                    *\n",
    "*           Laboratory of Computer and Information Science             *\n",
    "*                Rakentajanaukio 2 C, SF-02150 Espoo                   *\n",
    "*                              FINLAND                                 *\n",
    "*                                                                      *\n",
    "*                      Copyright (c) 1991-1995                         *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*  NOTE: This program package is copyrighted in the sense that it      *\n",
    "*  may be used for scientific purposes. The package as a whole, or     *\n",
    "*  parts thereof, cannot be included or used in any commercial         *\n",
    "*  application without written permission granted by its producents.   *\n",
    "*  No programs contained in this package may be copied for commercial  *\n",
    "*  distribution.                                                       *\n",
    "*                                                                      *\n",
    "*  All comments concerning this program package may be sent to the     *\n",
    "*  e-mail address 'lvq@nucleus.hut.fi'.                                *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "'''\n",
    "raw_ds.add_metadata(contents=license_txt, kind='LICENSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 13:59:09,964 - fetch - DEBUG - No file_name specified. Inferring lvq_pak-3.1.tar from URL\n",
      "2018-10-19 13:59:09,967 - fetch - DEBUG - lvq_pak-3.1.tar already exists and hash is valid\n",
      "2018-10-19 13:59:09,968 - fetch - DEBUG - lvq-pak.readme already exists and hash is valid\n",
      "2018-10-19 13:59:09,969 - fetch - DEBUG - Creating lvq-pak.license from `contents` string\n",
      "2018-10-19 13:59:09,970 - fetch - DEBUG - lvq-pak.license already exists and hash is valid\n",
      "2018-10-19 13:59:09,972 - fetch - DEBUG - lvq-pak.readme exists, but no hash to check. Setting to sha1:138b69cc0b4e02950cec5833752e50a54d36fd0f\n",
      "2018-10-19 13:59:09,972 - fetch - DEBUG - Creating lvq-pak.license from `contents` string\n",
      "2018-10-19 13:59:09,974 - fetch - DEBUG - lvq-pak.license exists, but no hash to check. Setting to sha1:e5f53b172926d34cb6a49877be49ee08bc4d51c1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 13:59:10,682 - datasets - DEBUG - Raw Dataset lvq-pak is already unpacked. Skipping\n"
     ]
    }
   ],
   "source": [
    "unpack_dir = raw_ds.unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kjell/Documents/devel/git/bus_number/data/interim/lvq-pak\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lvq-pak.license', 'lvq-pak.readme', 'lvq_pak-3.1']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'{unpack_dir}')\n",
    "list_dir(unpack_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvq-pak']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_raw_dataset(raw_ds)   # Add this raw dataset to the catalog\n",
    "workflow.available_raw_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Mark and F-MNIST\n",
    "For this excercise, you are going to help Mark build a `RawDataset` out of the Fashion-MNIST files.\n",
    "\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) is available from GitHub. Looking at the documentation there, we see that the raw data is distributed as a set of 4 files. The git repo specifies the checksums of these files:\n",
    "\n",
    "| Name  | Content | Examples | Size | Link | MD5 Checksum|\n",
    "| --- | --- |--- | --- |--- |--- |\n",
    "| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n",
    "| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n",
    "| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n",
    "| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n",
    "\n",
    "Your mission is to build a `RawDataset` that downloads these raw files and verifies that the hash values are as expected. You should make sure to include metadata in this `RawDataset`, including **description** (DESCR) and **license** (LICENSE) inforation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Raw Data\n",
    "How do we turn raw data into something useful? There are 2 steps:\n",
    "1. Write a function to extract meaningful `data` (and optionally, `target`) objects from your raw files, and\n",
    "2. Wrap this function in the form of a **processing function**\n",
    "\n",
    "#### Processing Function Template\n",
    "A processing function is a function that \n",
    "* takes at least 2 keyword arguments as input: `dataset_name` (a string) and `metadata` (a dict).\n",
    "* Returns a dictionary with the following keys: `dataset_name`, `data`, `target` (optional), and `metadata`\n",
    "Here's a template:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(dataset_name='raw_data', metadata=None):\n",
    "    \"\"\"Process a raw dataset object\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    extract_func: function returning tuple: (data, target)\n",
    "        Function to extract data and target\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    data, target = None, None\n",
    "\n",
    "    # Generate `data` and `target` info\n",
    "    #    data, target = extract_func()\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Processing lvq-pak data\n",
    "Bjørn has successfully fetched and extracted the lvq-pak data. Now he is ready to process it into `data` and `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 14:38:03,528 - datasets - DEBUG - unpack() called before fetch()\n",
      "2018-10-19 14:38:03,529 - fetch - DEBUG - No file_name specified. Inferring lvq_pak-3.1.tar from URL\n",
      "2018-10-19 14:38:03,533 - fetch - DEBUG - lvq_pak-3.1.tar already exists and hash is valid\n",
      "2018-10-19 14:38:03,535 - fetch - DEBUG - lvq-pak.readme already exists and hash is valid\n",
      "2018-10-19 14:38:03,536 - fetch - DEBUG - Creating lvq-pak.license from `contents` string\n",
      "2018-10-19 14:38:03,538 - fetch - DEBUG - lvq-pak.license already exists and hash is valid\n",
      "2018-10-19 14:38:03,539 - fetch - DEBUG - lvq-pak.readme already exists and hash is valid\n",
      "2018-10-19 14:38:03,539 - fetch - DEBUG - Creating lvq-pak.license from `contents` string\n",
      "2018-10-19 14:38:03,542 - fetch - DEBUG - lvq-pak.license already exists and hash is valid\n",
      "2018-10-19 14:38:03,563 - fetch - DEBUG - Extracting lvq_pak-3.1.tar\n",
      "2018-10-19 14:38:03,564 - fetch - DEBUG - Copying lvq-pak.readme\n",
      "2018-10-19 14:38:03,566 - fetch - DEBUG - Copying lvq-pak.license\n",
      "2018-10-19 14:38:03,568 - fetch - DEBUG - Copying lvq-pak.readme\n",
      "2018-10-19 14:38:03,569 - fetch - DEBUG - Copying lvq-pak.license\n"
     ]
    }
   ],
   "source": [
    "raw_ds = RawDataset.from_name('lvq-pak')    # load it from the catalog\n",
    "unpack_dir = raw_ds.unpack()                # Find the location of the unpacked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvq-pak.license', 'lvq-pak.readme', 'lvq_pak-3.1']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dir(unpack_dir) # what's the extracted data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy.c',\n",
       " 'balance.c',\n",
       " 'classify.c',\n",
       " 'cmatr.c',\n",
       " 'config.h',\n",
       " 'datafile.c',\n",
       " 'datafile.h',\n",
       " 'elimin.c',\n",
       " 'errors.h',\n",
       " 'eveninit.c',\n",
       " 'ex1.dat',\n",
       " 'ex2.dat',\n",
       " 'extract.c',\n",
       " 'fileio.c',\n",
       " 'fileio.h',\n",
       " 'knntest.c',\n",
       " 'labels.c',\n",
       " 'labels.h',\n",
       " 'lvq_pak.c',\n",
       " 'lvq_pak.h',\n",
       " 'lvq_rout.c',\n",
       " 'lvq_rout.h',\n",
       " 'lvq_run.c',\n",
       " 'lvqtrain.c',\n",
       " 'makefile.dos',\n",
       " 'makefile.unix',\n",
       " 'mcnemar.c',\n",
       " 'mindist.c',\n",
       " 'pick.c',\n",
       " 'README',\n",
       " 'sammon.c',\n",
       " 'setlabel.c',\n",
       " 'showlabs.c',\n",
       " 'stddev.c',\n",
       " 'VERSION',\n",
       " 'version.c',\n",
       " 'version.h']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dir(unpack_dir / 'lvq_pak-3.1')  # Files are extracted to a subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile_train = unpack_dir / 'lvq_pak-3.1' / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'lvq_pak-3.1' / 'ex2.dat'\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "# Example data from speech signal\n",
      "21.47 -19.90 -20.68 -6.73 13.67 -11.95 13.83 12.02 7.62 -6.15 -4.38 -2.91 4.80 -7.39 -3.54 -0.87 -5.02 -1.41 -2.33 2.12 A\n",
      "0.05 28.38 9.52 -11.30 3.11 -11.88 -2.90 -11.04 2.32 -13.80 1.71 -0.40 -1.36 3.91 3.21 -0.98 -0.14 -4.70 0.30 0.27 I\n",
      "-4.71 -4.61 -0.64 1.78 -1.48 5.98 12.55 -0.50 4.74 4.68 3.27 -0.36 9.24 3.39 -0.40 -1.59 0.94 2.17 -0.10 -0.45 #\n",
      "10.78 -22.31 -11.32 -10.92 10.96 -14.64 7.02 13.83 6.72 -7.99 -7.45 -3.20 8.45 2.76 -2.85 1.22 -6.60 -4.96 -1.42 0.57 A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.utils import head_file\n",
    "print(head_file(datafile_train)) # number of data columns, followed by comment, then space-delimited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "13.55 -12.07 -18.81 -6.13 10.66 -16.64 10.54 3.97 6.41 -8.01 -4.28 -3.37 7.96 -3.06 1.85 -5.28 1.05 3.87 1.77 -1.79 A\n",
      "0.28 22.76 8.26 -15.68 3.43 -13.39 -10.58 -14.71 3.96 -9.65 4.74 -9.55 0.90 7.25 4.69 -2.44 1.96 -4.32 2.48 1.46 I\n",
      "-3.51 -6.01 -0.47 -0.82 -0.38 -2.91 -1.35 0.48 1.88 3.00 4.11 7.21 3.36 7.48 2.37 -5.26 2.58 1.99 -1.09 -4.20 #\n",
      "9.76 -23.02 -12.69 -12.28 12.26 -15.74 10.01 8.30 1.95 -7.41 -0.68 -2.56 5.02 -1.56 -0.16 -1.87 -6.97 -0.08 0.51 2.00 A\n",
      "-2.85 -3.63 -0.89 -1.04 -3.26 4.13 4.37 5.18 2.79 -1.02 8.60 9.78 2.49 -2.63 -1.52 -1.26 2.27 -0.17 2.05 1.54 #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(head_file(datafile_test)) # similar, but no comment header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True):\n",
    "    \"\"\"Read an space-delimited file\n",
    "    \n",
    "    Data is space-delimited. Last column is the (string) label for the data\n",
    "\n",
    "    Note: we can't use automatic comment detection, as `#` characters are also used as data labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skiprows: None or list\n",
    "        list of rows to skip when reading the file.\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class (target) label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_table(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1962, 20), (1962,))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lvq_pak(dataset_name='lvq-pak', metadata=None, kind='all'):\n",
    "    \"\"\"Process LVQ-data object\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    extract_func: function returning tuple: (data, target)\n",
    "        Function to extract data and target\n",
    "    kind: {'train', 'test', 'all'}\n",
    "        Whether to return training set, test set, or everything. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    untar_dir = interim_data_path / dataset_name\n",
    "    unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "    elif kind == 'test':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "    elif kind == 'all':\n",
    "        data1, target1 = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "        data2, target2 = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'lvq-pak',\n",
       " 'metadata': {},\n",
       " 'data': array([['21.47', '-19.90', '-20.68', ..., '-1.41', '-2.33', '2.12'],\n",
       "        ['0.05', '28.38', '9.52', ..., '-4.70', '0.30', '0.27'],\n",
       "        ['-4.71', '-4.61', '-0.64', ..., '2.17', '-0.10', '-0.45'],\n",
       "        ...,\n",
       "        ['-2.63', '-6.59', '0.19', ..., '0.76', '0.89', '-3.48'],\n",
       "        ['5.35', '4.96', '18.75', ..., '-0.57', '0.00', '1.35'],\n",
       "        ['-0.37', '-5.27', '-1.74', ..., '3.48', '-0.90', '-1.00']],\n",
       "       dtype=object),\n",
       " 'target': array(['A', 'I', '#', ..., '#', 'Y', '#'], dtype=object)}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_lvq_pak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds.load_function = process_lvq_pak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 14:50:15,167 - datasets - DEBUG - Found cached Dataset for lvq-pak: 9237b9237ed5cfb499e7ba4c10788295b09b4659\n"
     ]
    }
   ],
   "source": [
    "ds = raw_ds.process() # Use the load_function to convert this RawDataset to a real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Dataset: <Dataset: lvq-pak, data.shape=(3924, 20), target.shape=(3924,), metadata=['descr', 'license', 'dataset_name', 'hash_type', 'data_hash', 'target_hash']>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Built Dataset: {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 14:57:49,630 - datasets - DEBUG - Found cached Dataset for lvq-pak: 1e177bc4e370b3cfcce9d862c8401ddb09075324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset: lvq-pak, data.shape=(1962, 20), target.shape=(1962,), metadata=['descr', 'license', 'dataset_name', 'hash_type', 'data_hash', 'target_hash']>\n"
     ]
    }
   ],
   "source": [
    "ds = raw_ds.process(kind=\"test\")  # Should be half the size\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE: Process Mark's F-MNIST Data\n",
    "In the last exercise, you fetched and unpacked F-MNIST data.\n",
    "Now it's time to process it into a usable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Dataset` and Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tour of the Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complicated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Data: The Punchline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bus_number]",
   "language": "python",
   "name": "conda-env-bus_number-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
