{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Reproducible Data\n",
    "*\"Raw Data is Read Only. Sing it with me\"*\n",
    "\n",
    "\n",
    "* RawDataset\n",
    "  * Fetching + Unpack\n",
    "      * Example 1: lvq-pak\n",
    "      * exercise: fmnist\n",
    "  * Attaching metadata\n",
    "      * Example 2: lvq-pak\n",
    "      * exercise: fmnist\n",
    "  * Processing data\n",
    "    * Process into data, (optionally, target)\n",
    "    * create a process_my_dataset() function\n",
    "        * Example 3: lvq-pak\n",
    "        * Exercise: fmnist\n",
    "  * Save the raw dataset to the raw dataset catalog\n",
    "      * the workflow module\n",
    "      * example: adding lvq-pak\n",
    "      * exercise: fmnist\n",
    "\n",
    "* Datasets and Data Transformers\n",
    "    * Create a transformer to produce a `Dataset` from the RawDataset\n",
    "    * Add this dataset to the catalog\n",
    "    * Load the dataset. \n",
    "    * Verify we get the same dataset when doing raw.process(), Dataset.from_raw, and Dataset.load() (look at hash)\n",
    "        * example: lvq-pak\n",
    "        * exercise: fmnist_test, fmnist_train\n",
    "    \n",
    "    * More Complicated Transformers\n",
    "        * Example: 80/20 Train/Test Split on lvq-pak\n",
    "        * Exercise: merge labels on lvq-pak (hierarchial categories)\n",
    "        * Exercise: merge labels on fmnist\n",
    "        \n",
    "* Punchline: \n",
    "  * delete all the files  (raw, interim, processed).\n",
    "  * make clean_raw, clean_cache, clean_processed, (clean_data?) `make data`\n",
    "  * look: same hashes as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Split this into 2 notebooks: lvq-pak and fmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `RawDataset`\n",
    "The `RawDataset` object handles downloading, unpacking, and processing raw data files, and serves as a container for some basic metadata, including **documentation** and **license** information.\n",
    "\n",
    "\n",
    "\n",
    "Raw data files are downloaded to  `paths.raw_data_path`.\n",
    " Cache files and unpacked raw files are saved to `paths.interim_data_path`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and Unpacking Raw Data\n",
    "\n",
    "#### LVQ-Pak: A Finnish Phonetic dataset\n",
    "\n",
    "The Learning Vector Quantization (lvq-pak) project includes a simple Finnish phonetic dataset\n",
    "consisting 20-dimensional Mel Frequency Cepstrum Coefficients (MFCCs) labelled with target phoneme information. Our goal is to explore this dataset, process it into a useful form, and make it a part of a reproducible data science workflow. The project can be found at: http://www.cis.hut.fi/research/lvq_pak/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: \n",
    "For this example, we are going create a `RawDataset` by:\n",
    "1. Downloading and unpacking the raw data files. \n",
    "2. Generate (and record) hash values for these files.\n",
    "2. Add relevant LICENSE and DESCR (description) metadata to this RawDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset\n",
    "from src.utils import list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = RawDataset('lvq-pak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds.add_url(\"http://www.cis.hut.fi/research/lvq_pak/lvq_pak-3.1.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds.add_url(\"http://www.cis.hut.fi/research/lvq_pak/README\",\n",
    "               file_name='lvq-pak.readme', name='DESCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "license_txt = '''\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*                              LVQ_PAK                                 *\n",
    "*                                                                      *\n",
    "*                                The                                   *\n",
    "*                                                                      *\n",
    "*                   Learning  Vector  Quantization                     *\n",
    "*                                                                      *\n",
    "*                          Program  Package                            *\n",
    "*                                                                      *\n",
    "*                   Version 3.1 (April 7, 1995)                        *\n",
    "*                                                                      *\n",
    "*                          Prepared by the                             *\n",
    "*                    LVQ Programming Team of the                       *\n",
    "*                 Helsinki University of Technology                    *\n",
    "*           Laboratory of Computer and Information Science             *\n",
    "*                Rakentajanaukio 2 C, SF-02150 Espoo                   *\n",
    "*                              FINLAND                                 *\n",
    "*                                                                      *\n",
    "*                      Copyright (c) 1991-1995                         *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*  NOTE: This program package is copyrighted in the sense that it      *\n",
    "*  may be used for scientific purposes. The package as a whole, or     *\n",
    "*  parts thereof, cannot be included or used in any commercial         *\n",
    "*  application without written permission granted by its producents.   *\n",
    "*  No programs contained in this package may be copied for commercial  *\n",
    "*  distribution.                                                       *\n",
    "*                                                                      *\n",
    "*  All comments concerning this program package may be sent to the     *\n",
    "*  e-mail address 'lvq@nucleus.hut.fi'.                                *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "'''\n",
    "raw_ds.add_metadata(contents=license_txt, kind='LICENSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "raw_ds.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_dir = raw_ds.unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{unpack_dir}')\n",
    "list_dir(unpack_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_raw_dataset(raw_ds)   # Add this raw dataset to the catalog\n",
    "workflow.available_raw_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Mark and F-MNIST\n",
    "For this excercise, you are going to help Mark build a `RawDataset` out of the Fashion-MNIST files.\n",
    "\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) is available from GitHub. Looking at the documentation there, we see that the raw data is distributed as a set of 4 files. The git repo specifies the checksums of these files:\n",
    "\n",
    "| Name  | Content | Examples | Size | Link | MD5 Checksum|\n",
    "| --- | --- |--- | --- |--- |--- |\n",
    "| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n",
    "| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n",
    "| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n",
    "| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n",
    "\n",
    "Your mission is to build a `RawDataset` that downloads these raw files and verifies that the hash values are as expected. You should make sure to include metadata in this `RawDataset`, including **description** (DESCR) and **license** (LICENSE) inforation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Raw Data\n",
    "How do we turn raw data into something useful? There are 2 steps:\n",
    "1. Write a function to extract meaningful `data` (and optionally, `target`) objects from your raw files, and\n",
    "2. Wrap this function in the form of a **processing function**\n",
    "\n",
    "#### Processing Function Template\n",
    "A processing function is a function that \n",
    "* takes at least 2 keyword arguments as input: `dataset_name` (a string) and `metadata` (a dict).\n",
    "* Returns a dictionary with the following keys: `dataset_name`, `data`, `target` (optional), and `metadata`\n",
    "Here's a template:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(dataset_name='raw_data', metadata=None):\n",
    "    \"\"\"Process a raw dataset object\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    extract_func: function returning tuple: (data, target)\n",
    "        Function to extract data and target\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    data, target = None, None\n",
    "\n",
    "    # Generate `data` and `target` info\n",
    "    #    data, target = extract_func()\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Processing lvq-pak data\n",
    "Bjørn has successfully fetched and extracted the lvq-pak data. Now he is ready to process it into `data` and `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = RawDataset.from_name('lvq-pak')    # load it from the catalog\n",
    "unpack_dir = raw_ds.unpack()                # Find the location of the unpacked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(unpack_dir) # what's the extracted data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(unpack_dir / 'lvq_pak-3.1')  # Files are extracted to a subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_train = unpack_dir / 'lvq_pak-3.1' / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'lvq_pak-3.1' / 'ex2.dat'\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import head_file\n",
    "print(head_file(datafile_train)) # number of data columns, followed by comment, then space-delimited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(head_file(datafile_test)) # similar, but no comment header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True):\n",
    "    \"\"\"Read an space-delimited file\n",
    "    \n",
    "    Data is space-delimited. Last column is the (string) label for the data\n",
    "\n",
    "    Note: we can't use automatic comment detection, as `#` characters are also used as data labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skiprows: None or list\n",
    "        list of rows to skip when reading the file.\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class (target) label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_table(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lvq_pak(dataset_name='lvq-pak', metadata=None, kind='all'):\n",
    "    \"\"\"Process LVQ-data object\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    extract_func: function returning tuple: (data, target)\n",
    "        Function to extract data and target\n",
    "    kind: {'train', 'test', 'all'}\n",
    "        Whether to return training set, test set, or everything. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    untar_dir = interim_data_path / dataset_name\n",
    "    unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "    elif kind == 'test':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "    elif kind == 'all':\n",
    "        data1, target1 = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "        data2, target2 = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_lvq_pak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds.load_function = process_lvq_pak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = raw_ds.process() # Use the load_function to convert this RawDataset to a real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Built Dataset: {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = raw_ds.process(kind=\"test\")  # Should be half the size\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE: Process Mark's F-MNIST Data\n",
    "In the last exercise, you fetched and unpacked F-MNIST data.\n",
    "Now it's time to process it into a usable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Dataset` and Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tour of the Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complicated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Data: The Punchline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bus_number]",
   "language": "python",
   "name": "conda-env-bus_number-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
