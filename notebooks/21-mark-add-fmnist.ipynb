{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark's Problem: Unsupervised Learning\n",
    "\n",
    "Mark regularly gets handed files full of fashion images, labelled by category. He wants to know how he can use this to help keep up with the latest trends for the magazine.\n",
    "\n",
    "For now, he's interested in producing a visualization of the various categories so that he can learn more about them. He's hoping his these explorations will eventually help him speed up the process of sorting through what he gets sent to review every week. \n",
    "\n",
    "But first, he has to put this data in a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset, Dataset\n",
    "from src.utils import list_dir\n",
    "from src.paths import raw_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are developing in a module, it's really handy to have these lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see debug-level logging in the notebook. Here's the incantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Datasets! Practice Makes Perfect. \n",
    "Acually, practice just makes permanent. **Perfect practice** makes perfect, but we digress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and processing the Fashion-MNIST (FMNIST) Dataset\n",
    "\n",
    "\n",
    "Recall that our approach to building a usable dataset is:\n",
    "\n",
    "1. Assemble the raw data files. Generate (and record) hashes to ensure the validity of these files.\n",
    "2. Add LICENSE and DESCR (description) metadata to make the raw data usable for other people, and\n",
    "3. Write a function to process the raw data into a usable format (for us, a `Dataset` object)\n",
    "4. Write transformation functions on `Dataset` objects that fit our data munging into an automated reproducible workflow. \n",
    "\n",
    "In practice, that means:\n",
    "\n",
    "* Create a `RawDataset`\n",
    "    * `add_url()`: give instructions for how to `fetch` your data and add a `DESCR` and `LICENSE`\n",
    "    * `add_process()`: add a function that knows how to process your specific dataset\n",
    "* `workflow.add_raw_dataset()`: add the `RawDataset` to your `workflow`\n",
    "* Transform your `Dataset`\n",
    "    * (Optionally add a `transformer` function to the `workflow`)\n",
    "    * `workflow.add_transformer()`: further transform your data. \n",
    "* Run `make data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the FMNIST GitHub documentation, we see that the raw data is distributed as a set of 4 files. \n",
    "\n",
    "| Name  | Content | Examples | Size | Link | MD5 Checksum|\n",
    "| --- | --- |--- | --- |--- |--- |\n",
    "| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n",
    "| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n",
    "| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n",
    "| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give our dataset a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"f-mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Check Hashes\n",
    "Because Zalando are excellent data citizens, they have conveniently given us MD5 hashes that we can verify when we download this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the log level to DEBUG so we can see what's going on\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the raw files  and their hashes\n",
    "data_site = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com'\n",
    "file_list = [\n",
    "    ('train-images-idx3-ubyte.gz','8d4fb7e6c68d591d4c3dfef9ec88bf0d'),\n",
    "    ('train-labels-idx1-ubyte.gz','25c81989df183df01b3e8a0aad5dffbe'),\n",
    "    ('t10k-images-idx3-ubyte.gz', 'bef4ecab320f06d8554ea6380940ec79'),\n",
    "    ('t10k-labels-idx1-ubyte.gz', 'bb300cfdad3c16e7a12a480ee83cd310'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = RawDataset(dataset_name)\n",
    "for file, hashval in file_list:\n",
    "    url = f\"{data_site}/{file}\"\n",
    "    fmnist.add_url(url=url, hash_type='md5', hash_value=hashval)\n",
    "# Download and check the hashes\n",
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget the License and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy case. Zalando are good data citizens, so their data License is directly available from\n",
    "# their Raw Data Repo on github\n",
    "\n",
    "# Notice we tag this data with the name `LICENSE`\n",
    "fmnist.add_url(url='https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/LICENSE',\n",
    "            name='LICENSE', file_name=f'{dataset_name}.license')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the raw data look like?\n",
    "# Where did I get it from? \n",
    "# What format is it in?\n",
    "# What should it look like when it's processed?\n",
    "fmnist_readme = '''\n",
    "Fashion-MNIST\n",
    "=============\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 70000\n",
    "    :Number of Attributes: 728\n",
    "    :Attribute Information: 28x28 8-bit greyscale image\n",
    "    :Missing Attribute Values: None\n",
    "    :Creator: Zalando\n",
    "    :Date: 2017\n",
    "\n",
    "This is a copy of Zalando's Fashion-MNIST [F-MNIST] dataset:\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original [MNIST] dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "References\n",
    "----------\n",
    "  - [F-MNIST] Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\n",
    "    Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv:1708.07747\n",
    "  - [MNIST] The MNIST Database of handwritten digits. Yann LeCun, Corinna Cortes,\n",
    "    Christopher J.C. Burges. http://yann.lecun.com/exdb/mnist/\n",
    "'''\n",
    "\n",
    "fmnist.add_metadata(kind=\"DESCR\", contents=fmnist_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, most unpacking can be handled automagically. Just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a `RawDataset` into a usable `Dataset`\n",
    "\n",
    "Recall that we need to write a processing function and add it to our `RawDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the raw data\n",
    "Finally, we need to convert the raw data into usable `data` and `target` vectors.\n",
    "The code at https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py tells us how to do that. Having a look at the sample code, we notice that we need numpy. How do we add this to the environment?\n",
    "* Add it to `environment.yml`\n",
    "* `make requirements`\n",
    "\n",
    "Once we have done this, we can do the following processing and setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "unpack_path = fmnist.unpack()\n",
    "kind = \"train\"\n",
    "\n",
    "label_path = unpack_path / f\"{kind}-labels-idx1-ubyte\"\n",
    "with open(label_path, 'rb') as fd:\n",
    "    target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "dataset_path = unpack_path / f\"{kind}-images-idx3-ubyte\"\n",
    "with open(dataset_path, 'rb') as fd:\n",
    "    data = np.frombuffer(fd.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)\n",
    "\n",
    "print(f'Data: {data.shape}, Target: {target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a `Dataset`\n",
    "\n",
    "Time to build a processing function. Recall that a processing function produces a dictionary of kwargs that can be used as a `Dataset` constructor:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset\n",
    "help(Dataset.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewriting the sample code into the framework gives us this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file -a ../src/data/localdata.py\n",
    "#__all__ += ['process_mnist']\n",
    "\n",
    "def process_mnist(dataset_name='mnist', kind='train', metadata=None):\n",
    "    '''\n",
    "    Load the MNIST dataset (or a compatible variant; e.g. F-MNIST)\n",
    "\n",
    "    dataset_name: {'mnist', 'f-mnist'}\n",
    "        Which variant to load\n",
    "    kind: {'train', 'test'}\n",
    "        Dataset comes pre-split into training and test data.\n",
    "        Indicates which dataset to load\n",
    "    metadata: dict\n",
    "        Additional metadata fields will be added to this dict.\n",
    "        'kind': value of `kind` used to generate a subset of the data\n",
    "    '''\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "        \n",
    "    if kind == 'test':\n",
    "        kind = 't10k'\n",
    "\n",
    "    label_path = interim_data_path / dataset_name / f\"{kind}-labels-idx1-ubyte\"\n",
    "    with open(label_path, 'rb') as fd:\n",
    "        target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "    dataset_path = interim_data_path / dataset_name / f\"{kind}-images-idx3-ubyte\"\n",
    "    with open(dataset_path, 'rb') as fd:\n",
    "        data = np.frombuffer(fd.read(), dtype=np.uint8,\n",
    "                                       offset=16).reshape(len(target), 784)\n",
    "    metadata['subset'] = kind\n",
    "    \n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "        'metadata': metadata,\n",
    "    }\n",
    "    return dset_opts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add this process function to the built in workflow in order to automate `Dataset` creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.data.localdata import process_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.unpack(force=True)\n",
    "fmnist.load_function = partial(process_mnist, dataset_name='f-mnist')\n",
    "ds = fmnist.process(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.shape, ds.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add this Dataset to the master dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Raw Dataset to the master list of Raw Datasets\n",
    "workflow.add_raw_dataset(fmnist)\n",
    "workflow.available_raw_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pair of Datasets from this Raw Dataset, by specifying different options for the RawDataset creation\n",
    "for kind in ['train', 'test']:\n",
    "    workflow.add_transformer(from_raw=fmnist.name, raw_dataset_opts={'kind':kind}, \n",
    "                             output_dataset=f\"{fmnist.name}_{kind}\")\n",
    "\n",
    "workflow.get_transformer_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transforms and save the resulting Datasets. This is the same as doing a `make data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "workflow.make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load these datsets by name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load(\"f-mnist_test\")\n",
    "print(f\"Data:{ds.data.shape}, Target:{ds.target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load(\"f-mnist_train\")\n",
    "print(f\"Data:{ds.data.shape}, Target:{ds.target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget: check in your changes  using `git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check in the generated `raw_datasets.json`, `transformer_list.json` in to source code control\n",
    "* do a `make data`\n",
    "* add tests if you haven't yet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Mark is well on his way to doing data science on his fashion data. In this example, he:\n",
    "* Created a `RawDataset` consisting of 4 raw data files\n",
    "* Checked the hashes of these files against known (published) values\n",
    "* Added license and description metadata\n",
    "* Added a processing function to parse the contents of these raw data files into a usable format, and\n",
    "* Created \"test\" and \"train\" variants of a `Dataset` object from this `RawDataset`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.data.localdata import process_mnist\n",
    "\n",
    "# Create a RawDataset from known hashes\n",
    "fmnist = RawDataset('f-mnist')\n",
    "data_site = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com'\n",
    "file_list = [\n",
    "    ('train-images-idx3-ubyte.gz','8d4fb7e6c68d591d4c3dfef9ec88bf0d'),\n",
    "    ('train-labels-idx1-ubyte.gz','25c81989df183df01b3e8a0aad5dffbe'),\n",
    "    ('t10k-images-idx3-ubyte.gz', 'bef4ecab320f06d8554ea6380940ec79'),\n",
    "    ('t10k-labels-idx1-ubyte.gz', 'bb300cfdad3c16e7a12a480ee83cd310'),\n",
    "]\n",
    "for file, hashval in file_list:\n",
    "    fmnist.add_url(url=f\"{data_site}/{file}\", hash_type='md5', hash_value=hashval)\n",
    "# Add metadata and processing functions\n",
    "fmnist.add_url(url='https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/LICENSE',\n",
    "               name='LICENSE', file_name=f'{dataset_name}.license')\n",
    "fmnist.add_metadata(kind=\"DESCR\", contents=fmnist_readme)\n",
    "fmnist.load_function = partial(process_mnist, dataset_name='f-mnist')\n",
    "workflow.add_raw_dataset(fmnist)\n",
    "workflow.make_raw()\n",
    "\n",
    "# Add Datasets (directly from raw)\n",
    "for kind in ['train', 'test']:\n",
    "    workflow.add_transformer(from_raw=fmnist.name, raw_dataset_opts={'kind':kind}, \n",
    "                             output_dataset=f\"{fmnist.name}_{kind}\")\n",
    "workflow.make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
