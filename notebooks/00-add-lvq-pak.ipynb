{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset, Dataset\n",
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data is Read Only: Sing it with me!\n",
    "\n",
    "How do you asseble raw data into a usable dataset? How do you make sure your raw datasets are reproducible?\n",
    "We are going to work through the process of collecting raw data files (using an object called a `RawDataset`), and converting them into something useful for our reproducible data science workflow (the `Dataset` object).\n",
    "\n",
    "## Bjørn's Problem: Supervised Learning\n",
    "\n",
    "Bjørn employs a large number of Finnish line cooks. He can’t understand a word they say.\n",
    "\n",
    "Bjørn needs a trained model to do real-time translation from Finnish to Swedish.\n",
    "\n",
    "Bjørn has decided to start with the Finnish phoneme dataset shipped with a project called lvq-pak. His objective is to train three different models, and choose the one with the best overall accuracy score.\n",
    "\n",
    "\n",
    "### LVQ-Pak: A Finnish Phonetic dataset\n",
    "\n",
    "The Learning Vector Quantization (lvq-pak) project includes a simple Finnish phonetic dataset\n",
    "consisting 20-dimensional Mel Frequency Cepstrum Coefficients (MFCCs) labelled with target phoneme information. Our goal is to explore this dataset, process it into a useful form, and make it a part of a reproducible data science workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='lvq-pak'  # Naming things: the hardest problem in computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lvq-pak is shipped as a source code tarball. The data files are included as textfiles within that download. Our first goal is to retrieve these source files, and record some metadata about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Grab the source code package\n",
    "We can add a file to the RawDataset using its `add_url()` method. If we know the hash of this file, we should include it here, and it will be checked on download. If not, one will be computed from this download and used for comparison on subsequent downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak = RawDataset(dataset_name)\n",
    "lvq_pak.add_url(url=\"http://www.cis.hut.fi/research/lvq_pak/lvq_pak-3.1.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some additional options to `fetch_url()` we may wish to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function add_url in module src.data.datasets:\n",
      "\n",
      "add_url(self, url=None, hash_type='sha1', hash_value=None, name=None, file_name=None)\n",
      "    Add a URL to the file list\n",
      "    \n",
      "    hash_type: {'sha1', 'md5', 'sha256'}\n",
      "    hash_value: string or None\n",
      "        if None, hash will be computed from downloaded file\n",
      "    file_name: string or None\n",
      "        Name of downloaded file. If None, will be the last component of the URL\n",
      "    url: string\n",
      "        URL to fetch\n",
      "    name: str\n",
      "        text description of this file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RawDataset.add_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hash_value` and `hash_type` should be self-evident.\n",
    "\n",
    "`file_name` is optional. If skipped, the name will be guessed from the URL. It's often nice to specify, however, so you can use this attribute to override the default guess.\n",
    "\n",
    "The `name` field can either be used to specify a text label to describe the dataset. It can also be used to tag two special pieces of metadata: `DESCR` and `LICENSE`, the downloaded (text) file will be used as the dataset description and license text, respectively.\n",
    "\n",
    "We will use this feature to add a `README` to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak.add_url(url='http://www.cis.hut.fi/research/lvq_pak/README',\n",
    "                file_name=f'{dataset_name}.readme',\n",
    "                name='DESCR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets should *always* have an explicit license. Reading the project documentation, we see a license in one of the textfiles. We can extract and use it via the `add_metadata()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "license_txt = '''\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*                              LVQ_PAK                                 *\n",
    "*                                                                      *\n",
    "*                                The                                   *\n",
    "*                                                                      *\n",
    "*                   Learning  Vector  Quantization                     *\n",
    "*                                                                      *\n",
    "*                          Program  Package                            *\n",
    "*                                                                      *\n",
    "*                   Version 3.1 (April 7, 1995)                        *\n",
    "*                                                                      *\n",
    "*                          Prepared by the                             *\n",
    "*                    LVQ Programming Team of the                       *\n",
    "*                 Helsinki University of Technology                    *\n",
    "*           Laboratory of Computer and Information Science             *\n",
    "*                Rakentajanaukio 2 C, SF-02150 Espoo                   *\n",
    "*                              FINLAND                                 *\n",
    "*                                                                      *\n",
    "*                      Copyright (c) 1991-1995                         *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*  NOTE: This program package is copyrighted in the sense that it      *\n",
    "*  may be used for scientific purposes. The package as a whole, or     *\n",
    "*  parts thereof, cannot be included or used in any commercial         *\n",
    "*  application without written permission granted by its producents.   *\n",
    "*  No programs contained in this package may be copied for commercial  *\n",
    "*  distribution.                                                       *\n",
    "*                                                                      *\n",
    "*  All comments concerning this program package may be sent to the     *\n",
    "*  e-mail address 'lvq@nucleus.hut.fi'.                                *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "'''\n",
    "lvq_pak.add_metadata(contents=license_txt, kind='LICENSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 13:46:49,649 - fetch - DEBUG - No file_name specified. Inferring lvq_pak-3.1.tar from URL\n",
      "2018-10-17 13:46:49,653 - fetch - DEBUG - lvq_pak-3.1.tar exists, but no hash to check. Setting to sha1:86024a871724e521341da0ffb783956e39aadb6e\n",
      "2018-10-17 13:46:49,656 - fetch - DEBUG - lvq-pak.readme exists, but no hash to check. Setting to sha1:138b69cc0b4e02950cec5833752e50a54d36fd0f\n",
      "2018-10-17 13:46:49,657 - fetch - DEBUG - Creating lvq-pak.license from `contents` string\n",
      "2018-10-17 13:46:49,661 - fetch - DEBUG - lvq-pak.license exists, but no hash to check. Setting to sha1:e5f53b172926d34cb6a49877be49ee08bc4d51c1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lvq_pak.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvq_pak-3.1.tar',\n",
       " 'fremont_bike.readme',\n",
       " 'f-mnist.license',\n",
       " 'f-mnist.readme',\n",
       " 'lvq-pak.readme',\n",
       " 'fremont_bike.license',\n",
       " 't10k-images-idx3-ubyte.gz',\n",
       " 'train-images-idx3-ubyte.gz',\n",
       " 'fremont.csv',\n",
       " 'train-labels-idx1-ubyte.gz',\n",
       " 'lvq-pak.license',\n",
       " 't10k-labels-idx1-ubyte.gz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils import list_dir\n",
    "from src.paths import raw_data_path\n",
    "list_dir(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "The next step is to write the importer that actually processes the data we will be using for this dataset.\n",
    "\n",
    "The important things to generate are `data` and `target` entries. A `metadata` is optional, but recommended if you want to save additional information about the dataset.\n",
    "\n",
    "Usually, this functionality gets bundled up into a function and added to `datasets.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 13:46:49,854 - fetch - DEBUG - Extracting lvq_pak-3.1.tar\n",
      "2018-10-17 13:46:49,857 - fetch - DEBUG - Copying lvq-pak.readme\n",
      "2018-10-17 13:46:49,861 - fetch - DEBUG - Copying lvq-pak.license\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lvq-pak.readme', 'lvq_pak-3.1', 'lvq-pak.license']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpack the file\n",
    "untar_dir = lvq_pak.unpack()\n",
    "list_dir(untar_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvq_rout.h',\n",
       " 'knntest.c',\n",
       " 'datafile.h',\n",
       " 'extract.c',\n",
       " 'lvq_pak.c',\n",
       " 'version.h',\n",
       " 'cmatr.c',\n",
       " 'fileio.c',\n",
       " 'setlabel.c',\n",
       " 'eveninit.c',\n",
       " 'config.h',\n",
       " 'mindist.c',\n",
       " 'elimin.c',\n",
       " 'pick.c',\n",
       " 'lvq_run.c',\n",
       " 'classify.c',\n",
       " 'lvqtrain.c',\n",
       " 'errors.h',\n",
       " 'stddev.c',\n",
       " 'README',\n",
       " 'labels.c',\n",
       " 'lvq_pak.h',\n",
       " 'balance.c',\n",
       " 'sammon.c',\n",
       " 'makefile.dos',\n",
       " 'makefile.unix',\n",
       " 'datafile.c',\n",
       " 'lvq_rout.c',\n",
       " 'accuracy.c',\n",
       " 'VERSION',\n",
       " 'ex2.dat',\n",
       " 'fileio.h',\n",
       " 'version.c',\n",
       " 'mcnemar.c',\n",
       " 'ex1.dat',\n",
       " 'labels.h',\n",
       " 'showlabs.c']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "list_dir(unpack_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the training and test datsets are stored in files named `ex1.dat` and `ex2.dat` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile_train = unpack_dir / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'ex2.dat'\n",
    "\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, the data format is space-delimited, with the class label included as the last column. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import head_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "# Example data from speech signal\n",
      "21.47 -19.90 -20.68 -6.73 13.67 -11.95 13.83 12.02 7.62 -6.15 -4.38 -2.91 4.80 -7.39 -3.54 -0.87 -5.02 -1.41 -2.33 2.12 A\n",
      "0.05 28.38 9.52 -11.30 3.11 -11.88 -2.90 -11.04 2.32 -13.80 1.71 -0.40 -1.36 3.91 3.21 -0.98 -0.14 -4.70 0.30 0.27 I\n",
      "-4.71 -4.61 -0.64 1.78 -1.48 5.98 12.55 -0.50 4.74 4.68 3.27 -0.36 9.24 3.39 -0.40 -1.59 0.94 2.17 -0.10 -0.45 #\n",
      "10.78 -22.31 -11.32 -10.92 10.96 -14.64 7.02 13.83 6.72 -7.99 -7.45 -3.20 8.45 2.76 -2.85 1.22 -6.60 -4.96 -1.42 0.57 A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(head_file(datafile_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the datafile consists of a single line containing the dimension of the data, a comment, and then 21 space-delimited columns, the final column being the target class label. \n",
    "\n",
    "**Note:** We have to be a little careful importing the data, because '#' is used both as the comment delimiter, and as a class label.\n",
    "\n",
    "Fortunately, we have a helper function for this. We will get a little cheeky and skip the first 2 lines (hoping there are no other comments). The documentation also says ther are 1962 entries in each of the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True):\n",
    "    \"\"\"Read an space-delimited file\n",
    "\n",
    "    skiprows: list of rows to skip when reading the file.\n",
    "\n",
    "    Note: we can't use automatic comment detection, as\n",
    "    `#` characters are also used as data labels.\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_table(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1962, 20), (1962,), (1962, 20), (1962,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data2, target2 = read_space_delimited(datafile_test, skiprows=[0])\n",
    "\n",
    "data.shape, target.shape, data2.shape, target2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'I', '#', ..., '#', 'Y', '#'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work, so let's wrap this functionality up into a processing function.\n",
    "By convention, a processing function takes a `dataset_name`, and any other options that may be useful for reading the data, and returns a dictionary that matches the `Dataset` constructor signature.\n",
    "\n",
    "We will place this function in `localdata.py`, (and add it to `__all__`) to make it visible to our dataset code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file ../src/data/localdata.py\n",
    "\"\"\"\n",
    "Custom dataset processing/generation functions should be added to this file\n",
    "\"\"\"\n",
    "\n",
    "from src.data.utils import read_space_delimited, normalize_labels\n",
    "from src.paths import interim_data_path\n",
    "import numpy as np\n",
    "\n",
    "__all__ = ['process_lvq_pak']\n",
    "\n",
    "def process_lvq_pak(dataset_name='lvq-pak', kind='all', numeric_labels=True, metadata=None):\n",
    "    \"\"\"\n",
    "    kind: {'test', 'train', 'all'}, default 'all'\n",
    "    numeric_labels: boolean (default: True)\n",
    "        if set, target is a vector of integers, and label_map is created in the metadata\n",
    "        to reflect the mapping to the string targets\n",
    "    \"\"\"\n",
    "    \n",
    "    untar_dir = interim_data_path / dataset_name\n",
    "    unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "    elif kind == 'test':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "    elif kind == 'all':\n",
    "        data1, target1 = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "        data2, target2 = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    if numeric_labels:\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        mapped_target, label_map = normalize_labels(target)\n",
    "        metadata['label_map'] = label_map\n",
    "        target = mapped_target\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "    return dset_opts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: data=(1962, 20) target=(1962,)\n",
      "test: data=(1962, 20) target=(1962,)\n",
      "all: data=(3924, 20) target=(3924,)\n"
     ]
    }
   ],
   "source": [
    "from src.data.localdata import process_lvq_pak\n",
    "\n",
    "for kind in ['train', 'test', 'all']:\n",
    "    dset_opts = process_lvq_pak(kind=kind)\n",
    "    dset = Dataset(**dset_opts)\n",
    "    print(f'{kind}: data={dset.data.shape} target={dset.target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all looks good. Add this function to the `RawDataset`, and add the `RawDataset` to the global dataset list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak.load_function = process_lvq_pak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_raw_dataset(lvq_pak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, re-load the dataset and save a copy of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 13:46:51,475 - fetch - DEBUG - No file_name specified. Inferring lvq_pak-3.1.tar from URL\n",
      "2018-10-17 13:46:51,482 - fetch - DEBUG - lvq_pak-3.1.tar already exists and hash is valid\n",
      "2018-10-17 13:46:51,485 - fetch - DEBUG - lvq-pak.readme already exists and hash is valid\n",
      "2018-10-17 13:46:51,488 - fetch - DEBUG - Creating lvq-pak.license from `contents` string\n",
      "2018-10-17 13:46:51,491 - fetch - DEBUG - lvq-pak.license already exists and hash is valid\n",
      "2018-10-17 13:46:51,531 - fetch - DEBUG - Extracting lvq_pak-3.1.tar\n",
      "2018-10-17 13:46:51,536 - fetch - DEBUG - Copying lvq-pak.readme\n",
      "2018-10-17 13:46:51,540 - fetch - DEBUG - Copying lvq-pak.license\n",
      "2018-10-17 13:46:52,278 - datasets - DEBUG - Wrote Dataset Metadata: 2c0bb10a816a7d45cce45984f1d5f9007c0a1d16.metadata\n",
      "2018-10-17 13:46:52,305 - datasets - DEBUG - Wrote Dataset: 2c0bb10a816a7d45cce45984f1d5f9007c0a1d16.dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset: lvq-pak, data.shape=(3924, 20), target.shape=(3924,), metadata=['descr', 'license', 'dataset_name', 'label_map', 'hash_type', 'data_hash', 'target_hash']>\n"
     ]
    }
   ],
   "source": [
    "lvq = Dataset.from_raw(dataset_name, force=True)\n",
    "print(str(lvq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, we can create an empty transformer to convert this RawDataset into a Dataset and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_transformer(from_raw=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'output_dataset': 'f-mnist_train',\n",
       "  'raw_dataset_name': 'f-mnist',\n",
       "  'raw_dataset_opts': {'kind': 'train'}},\n",
       " {'output_dataset': 'f-mnist_test',\n",
       "  'raw_dataset_name': 'f-mnist',\n",
       "  'raw_dataset_opts': {'kind': 'test'}},\n",
       " {'output_dataset': 'lvq-pak', 'raw_dataset_name': 'lvq-pak'},\n",
       " {'raw_dataset_name': 'lvq-pak',\n",
       "  'transformations': [['train_test_split',\n",
       "    {'random_state': 6502, 'test_size': 0.2}]]},\n",
       " {'output_dataset': 'lvq-pak', 'raw_dataset_name': 'lvq-pak'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.get_transformer_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete dataset can be written to `processed_data_path`. A copy of just the metadata is also stored, so that it may be quickly checked without loading the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 13:46:53,410 - transform_data - INFO - Writing transformed Dataset: f-mnist_train\n",
      "2018-10-17 13:46:54,896 - transform_data - INFO - Writing transformed Dataset: f-mnist_test\n",
      "2018-10-17 13:46:55,019 - transform_data - INFO - Writing transformed Dataset: lvq-pak\n",
      "2018-10-17 13:46:55,506 - transformers - INFO - Writing Transformed Dataset: lvq-pak_train\n",
      "2018-10-17 13:46:55,800 - transformers - INFO - Writing Transformed Dataset: lvq-pak_test\n",
      "2018-10-17 13:46:55,917 - transform_data - INFO - Writing transformed Dataset: lvq-pak\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "workflow.apply_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_meta = Dataset.load(dataset_name, metadata_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lvq_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c87a90aee1ddadb50282e68b9f0155a74b6d7a61\n"
     ]
    }
   ],
   "source": [
    "print(lvq.DATA_HASH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even better, from now on, **anytime** we want to access this dataset, all we have to do now is to run `make data`, and then load the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_from_file = Dataset.load(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3924, 20)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lvq_from_file.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Since the tasks we have for Bjørn is to build a supervised model, we will undoubtably want to be able to create a train/test split. \n",
    "\n",
    "How can we script this into the Dataset creation process?\n",
    "\n",
    "We can  add a **transformer** that builds the train/test split. Since this is a transformation we want to do all the time, we already have one built in to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvq-pak_test', 'f-mnist_train', 'lvq-pak', 'lvq-pak_train', 'f-mnist_test']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index_to_date_time', 'pivot', 'train_test_split']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.available_transformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function split_dataset_test_train in module src.data.transformers:\n",
      "\n",
      "split_dataset_test_train(dset, dump_path=None, dump_metadata=True, force=True, create_dirs=True, **split_opts)\n",
      "    Transformer that performs a train/test split.\n",
      "    \n",
      "    This transformer passes `dset` intact, but creates and dumps two new\n",
      "    datasets as a side effect: {dset.name}_test and {dset.name}_train\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    dump_metadata: boolean\n",
      "        If True, also dump a standalone copy of the metadata.\n",
      "        Useful for checking metadata without reading\n",
      "        in the (potentially large) dataset itself\n",
      "    dump_path: path. (default: `processed_data_path`)\n",
      "        Directory where data will be dumped.\n",
      "    force: boolean\n",
      "        If False, raise an exception if any dunp files already exists\n",
      "        If True, overwrite any existing files\n",
      "    create_dirs: boolean\n",
      "        If True, `dump_path` will be created (if necessary)\n",
      "    **split_opts:\n",
      "        Remaining options will be passed to `train_test_split`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(workflow.available_transformers(keys_only=False)['train_test_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'output_dataset': 'f-mnist_train',\n",
       "  'raw_dataset_name': 'f-mnist',\n",
       "  'raw_dataset_opts': {'kind': 'train'}},\n",
       " {'output_dataset': 'f-mnist_test',\n",
       "  'raw_dataset_name': 'f-mnist',\n",
       "  'raw_dataset_opts': {'kind': 'test'}},\n",
       " {'output_dataset': 'lvq-pak', 'raw_dataset_name': 'lvq-pak'},\n",
       " {'raw_dataset_name': 'lvq-pak',\n",
       "  'transformations': [['train_test_split',\n",
       "    {'random_state': 6502, 'test_size': 0.2}]]},\n",
       " {'output_dataset': 'lvq-pak', 'raw_dataset_name': 'lvq-pak'},\n",
       " {'raw_dataset_name': 'lvq-pak',\n",
       "  'transformations': [['train_test_split',\n",
       "    {'random_state': 6502, 'test_size': 0.2}]]}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.get_transformer_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = [(\"train_test_split\", {'random_state':6502, 'test_size':0.2})]\n",
    "workflow.add_transformer(from_raw='lvq-pak',\n",
    "                         suppress_output=True,\n",
    "                         transformations=transform_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 13:46:58,898 - transform_data - INFO - Writing transformed Dataset: f-mnist_train\n",
      "2018-10-17 13:47:00,262 - transform_data - INFO - Writing transformed Dataset: f-mnist_test\n",
      "2018-10-17 13:47:00,353 - transform_data - INFO - Writing transformed Dataset: lvq-pak\n",
      "2018-10-17 13:47:00,784 - transformers - INFO - Writing Transformed Dataset: lvq-pak_train\n",
      "2018-10-17 13:47:01,053 - transformers - INFO - Writing Transformed Dataset: lvq-pak_test\n",
      "2018-10-17 13:47:01,171 - transform_data - INFO - Writing transformed Dataset: lvq-pak\n",
      "2018-10-17 13:47:01,546 - transformers - INFO - Writing Transformed Dataset: lvq-pak_train\n",
      "2018-10-17 13:47:01,935 - transformers - INFO - Writing Transformed Dataset: lvq-pak_test\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.INFO) # This step is a bit noisy, otherwise.\n",
    "workflow.apply_transforms()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there are now 2 new datasets: `lvq-pak_test` and `lvq-pak_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvq-pak_test', 'f-mnist_train', 'lvq-pak', 'lvq-pak_train', 'f-mnist_test']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the split data. The metadata now reflects the options that were used to generate this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 6502, 'test_size': 0.2}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load('lvq-pak_train')\n",
    "ds.SPLIT_OPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, any other munging that we want to do to our data can be done via a **transformer**. That way, our RawDataset stays **read-only**, and all of our munging is scripted and reproducible. Furthermore, we can easily access our transformed data whenever we need to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bus_number]",
   "language": "python",
   "name": "conda-env-bus_number-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
