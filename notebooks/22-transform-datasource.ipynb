{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Transforming Data Sources into Data\n",
    "“It is a capital mistake to theorize before one has data.” Sherlock Holmes, “A Study in Scarlett” (Arthur Conan Doyle).\n",
    "\n",
    "“If we have data, let’s look at data. If all we have are opinions, let’s go with mine.” – Jim Barksdale, former Netscape CEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning a `DataSource` into a `Dataset`\n",
    "How do we turn raw data sources into something useful? There are 2 steps:\n",
    "1. Write a function to extract meaningful `data` (and optionally, `target`) objects from your raw source files, and\n",
    "2. Wrap this function in the form of a **processing function**\n",
    "\n",
    "\n",
    "First, let's grab the dataset we created in the last notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a `DataSet` from the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow\n",
    "from src.data import DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvq-pak']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource.from_name('lvq-pak')    # load it from the catalog\n",
    "unpack_dir = dsrc.unpack()                # Find the location of the unpacked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "drwxr-xr-x 2 ava00088 users    0 Feb  6 17:31 .\r\n",
      "drwxr-xr-x 2 ava00088 users 4096 Feb  6 18:24 ..\r\n",
      "drwxr-xr-x 2 ava00088 users    0 Apr  6  1995 lvq_pak-3.1\r\n",
      "-rwxr-xr-x 1 ava00088 users 2483 Feb  7 13:53 lvq-pak.license\r\n",
      "-rwxr-xr-x 1 ava00088 users 4958 Feb  7 13:53 lvq-pak.readme\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la $unpack_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Function Template\n",
    "A **processing function** is a function that \n",
    "* takes at least 2 keyword arguments as input: `dataset_name` (a string) and `metadata` (a dict).\n",
    "* Returns a dictionary with the following keys: `dataset_name`, `data`, `target` (optional), and `metadata`\n",
    "\n",
    "Why a dictionary? Because the ouput of this function becomes the keyword arguments\n",
    "passed to the `Dataset` constructor.\n",
    "\n",
    "Here's a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(dataset_name='raw_data', metadata=None, extract_func=None, **kwargs):\n",
    "    \"\"\"Convert a raw DataSource files into a Dataset constructor dict\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    extract_func: function returning tuple: (data, target, metadata)\n",
    "    **kwargs: additional parameters to be passed to `extract_func`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    data, target = None, None\n",
    "    \n",
    "    if extract_function is None:\n",
    "        def extract_function(**kw):\n",
    "            return (data, target, metadata)\n",
    "  \n",
    "    # Generate `data` and `target` info\n",
    "    data, target, metadata = extract_func(metadata=metadata, **kwargs)\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Processing lvq-pak data\n",
    "Let's convert the lvq-pak data (introduced in the last section) into into `data` and `target` vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 762\r\n",
      "drwxr-xr-x 2 ava00088 users      0 Apr  6  1995 .\r\n",
      "drwxr-xr-x 2 ava00088 users      0 Feb  6 17:31 ..\r\n",
      "-rwxr-xr-x 1 ava00088 users   6358 Apr  6  1995 accuracy.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   7805 Apr  6  1995 balance.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   5577 Apr  6  1995 classify.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   7092 Apr  6  1995 cmatr.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   3797 Apr  6  1995 config.h\r\n",
      "-rwxr-xr-x 1 ava00088 users  28354 Apr  6  1995 datafile.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   4294 Apr  6  1995 datafile.h\r\n",
      "-rwxr-xr-x 1 ava00088 users   5044 Apr  6  1995 elimin.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   2626 Apr  6  1995 errors.h\r\n",
      "-rwxr-xr-x 1 ava00088 users   7122 Apr  6  1995 eveninit.c\r\n",
      "-rwxr-xr-x 1 ava00088 users 226894 Apr  6  1995 ex1.dat\r\n",
      "-rwxr-xr-x 1 ava00088 users 225948 Apr  6  1995 ex2.dat\r\n",
      "-rwxr-xr-x 1 ava00088 users   4226 Apr  6  1995 extract.c\r\n",
      "-rwxr-xr-x 1 ava00088 users  10101 Apr  6  1995 fileio.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   2896 Apr  6  1995 fileio.h\r\n",
      "-rwxr-xr-x 1 ava00088 users   6046 Apr  6  1995 knntest.c\r\n",
      "-rwxr-xr-x 1 ava00088 users  10459 Apr  6  1995 labels.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   2665 Apr  6  1995 labels.h\r\n",
      "-rwxr-xr-x 1 ava00088 users  15073 Apr  6  1995 lvq_pak.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   9279 Apr  6  1995 lvq_pak.h\r\n",
      "-rwxr-xr-x 1 ava00088 users  25023 Apr  6  1995 lvq_rout.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   1402 Apr  6  1995 lvq_rout.h\r\n",
      "-rwxr-xr-x 1 ava00088 users  34695 Apr  6  1995 lvq_run.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   8820 Apr  6  1995 lvqtrain.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   3141 Apr  6  1995 makefile.dos\r\n",
      "-rwxr-xr-x 1 ava00088 users   3238 Apr  6  1995 makefile.unix\r\n",
      "-rwxr-xr-x 1 ava00088 users   4768 Apr  6  1995 mcnemar.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   3957 Apr  6  1995 mindist.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   2900 Apr  6  1995 pick.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   4965 Apr  6  1995 README\r\n",
      "-rwxr-xr-x 1 ava00088 users  13025 Apr  6  1995 sammon.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   5020 Apr  6  1995 setlabel.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   3094 Apr  6  1995 showlabs.c\r\n",
      "-rwxr-xr-x 1 ava00088 users   3182 Apr  6  1995 stddev.c\r\n",
      "-rwxr-xr-x 1 ava00088 users     20 Apr  6  1995 VERSION\r\n",
      "-rwxr-xr-x 1 ava00088 users    207 Apr  6  1995 version.c\r\n",
      "-rwxr-xr-x 1 ava00088 users     68 Apr  6  1995 version.h\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la $unpack_dir/lvq_pak-3.1  # Files are extracted to a subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile_train = unpack_dir / 'lvq_pak-3.1' / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'lvq_pak-3.1' / 'ex2.dat'\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\r\n",
      "# Example data from speech signal\r\n",
      "21.47 -19.90 -20.68 -6.73 13.67 -11.95 13.83 12.02 7.62 -6.15 -4.38 -2.91 4.80 -7.39 -3.54 -0.87 -5.02 -1.41 -2.33 2.12 A\r\n",
      "0.05 28.38 9.52 -11.30 3.11 -11.88 -2.90 -11.04 2.32 -13.80 1.71 -0.40 -1.36 3.91 3.21 -0.98 -0.14 -4.70 0.30 0.27 I\r\n",
      "-4.71 -4.61 -0.64 1.78 -1.48 5.98 12.55 -0.50 4.74 4.68 3.27 -0.36 9.24 3.39 -0.40 -1.59 0.94 2.17 -0.10 -0.45 #\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 $datafile_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `datafile_train` (`ex1.dat`) appears to consists of:\n",
    "* the number of data columns, followed by\n",
    "* a comment line, then\n",
    "* space-delimited data\n",
    "\n",
    "**Wait!** There's a gotcha here. Look at the last entry in each row. That's the data label. In the last row, however, we see that `#` is used as a data label (easily confused for a comment). Be careful handling this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\r\n",
      "13.55 -12.07 -18.81 -6.13 10.66 -16.64 10.54 3.97 6.41 -8.01 -4.28 -3.37 7.96 -3.06 1.85 -5.28 1.05 3.87 1.77 -1.79 A\r\n",
      "0.28 22.76 8.26 -15.68 3.43 -13.39 -10.58 -14.71 3.96 -9.65 4.74 -9.55 0.90 7.25 4.69 -2.44 1.96 -4.32 2.48 1.46 I\r\n",
      "-3.51 -6.01 -0.47 -0.82 -0.38 -2.91 -1.35 0.48 1.88 3.00 4.11 7.21 3.36 7.48 2.37 -5.26 2.58 1.99 -1.09 -4.20 #\r\n",
      "9.76 -23.02 -12.69 -12.28 12.26 -15.74 10.01 8.30 1.95 -7.41 -0.68 -2.56 5.02 -1.56 -0.16 -1.87 -6.97 -0.08 0.51 2.00 A\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 $datafile_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `datafile_test` (`ex2.dat`) is similar, but has no comment header.\n",
    " Let's parse these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True, metadata=None):\n",
    "    \"\"\"Read an space-delimited file\n",
    "    \n",
    "    Data is space-delimited. Last column is the (string) label for the data\n",
    "\n",
    "    Note: we can't use automatic comment detection, as `#` characters are also\n",
    "    used as data labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skiprows: list-like, int or callable, optional\n",
    "        list of rows to skip when reading the file. See `pandas.read_csv`\n",
    "        entry on `skiprows` for more\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class (target) label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_csv(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1962, 20), (1962,), None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target, metadata = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lvq_pak(dataset_name='lvq-pak', metadata=None, kind='all'):\n",
    "    \"\"\"Process LVQ-data object\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    kind: {'train', 'test', 'all'}\n",
    "        Whether to return training set, test set, or everything. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    untar_dir = interim_data_path / dataset_name\n",
    "    unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target, metadata = read_space_delimited(unpack_dir / 'ex1.dat',\n",
    "                                                      skiprows=[0,1],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'test':\n",
    "        data, target, metadata = read_space_delimited(unpack_dir / 'ex2.dat',\n",
    "                                                      skiprows=[0],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'all':\n",
    "        data1, target1, metadata = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1],\n",
    "                                                        metadata=metadata)\n",
    "        data2, target2, metadata = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0],\n",
    "                                                        metadata=metadata)\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'lvq-pak',\n",
       " 'metadata': {},\n",
       " 'data': array([['21.47', '-19.90', '-20.68', ..., '-1.41', '-2.33', '2.12'],\n",
       "        ['0.05', '28.38', '9.52', ..., '-4.70', '0.30', '0.27'],\n",
       "        ['-4.71', '-4.61', '-0.64', ..., '2.17', '-0.10', '-0.45'],\n",
       "        ...,\n",
       "        ['-2.63', '-6.59', '0.19', ..., '0.76', '0.89', '-3.48'],\n",
       "        ['5.35', '4.96', '18.75', ..., '-0.57', '0.00', '1.35'],\n",
       "        ['-0.37', '-5.27', '-1.74', ..., '3.48', '-0.90', '-1.00']],\n",
       "       dtype=object),\n",
       " 'target': array(['A', 'I', '#', ..., '#', 'Y', '#'], dtype=object)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_lvq_pak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.load_function = process_lvq_pak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write this into the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_datasource(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvq-pak']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<Dataset: lvq-pak, data.shape=(3924, 20), target.shape=(3924,), metadata=['descr', 'license', 'dataset_name', 'hash_type', 'data_hash', 'target_hash']>\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = dsrc.process() # Use the load_function to convert this DataSource to a real Dataset\n",
    "str(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset: lvq-pak, data.shape=(3924, 20), target.shape=(3924,), metadata=['descr', 'license', 'dataset_name', 'hash_type', 'data_hash', 'target_hash']>\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset: lvq-pak, data.shape=(1962, 20), target.shape=(1962,), metadata=['descr', 'license', 'dataset_name', 'hash_type', 'data_hash', 'target_hash']>\n"
     ]
    }
   ],
   "source": [
    "ds = dsrc.process(kind=\"test\")  # Should be half the size\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Turn the F-MNIST `DataSource` into a `Dataset`\n",
    "In the last exercise, you fetched and unpacked F-MNIST data.\n",
    "Now it's time to process it into a `Dataset` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Dataset` and Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tour of the Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complicated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Data: The Punchline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
