{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Transforming Data Sources into Data\n",
    "“It is a capital mistake to theorize before one has data.” Sherlock Holmes, “A Study in Scarlett” (Arthur Conan Doyle).\n",
    "\n",
    "“If we have data, let’s look at data. If all we have are opinions, let’s go with mine.” – Jim Barksdale, former Netscape CEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning a `DataSource` into a `Dataset`\n",
    "How do we turn raw data sources into something useful? There are 2 steps:\n",
    "1. Write a function to extract meaningful `data` (and optionally, `target`) objects from your raw source files, and\n",
    "2. Wrap this function in the form of a **processing function**\n",
    "\n",
    "\n",
    "First, let's grab the dataset we created in the last notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a `DataSet` from the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow\n",
    "from src.data import DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource.from_name('lvq-pak')    # load it from the catalog\n",
    "unpack_dir = dsrc.unpack()                # Find the location of the unpacked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $unpack_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Function Template\n",
    "A **processing function** is a function that \n",
    "* takes at least 2 keyword arguments as input: `dataset_name` (a string) and `metadata` (a dict).\n",
    "* Returns a dictionary with the following keys: `dataset_name`, `data`, `target` (optional), and `metadata`\n",
    "\n",
    "Why a dictionary? Because the ouput of this function becomes the keyword arguments\n",
    "passed to the `Dataset` constructor.\n",
    "\n",
    "Here's a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(dataset_name='raw_data', metadata=None, extract_func=None, **kwargs):\n",
    "    \"\"\"Convert a raw DataSource files into a Dataset constructor dict\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    extract_func: function returning tuple: (data, target, metadata)\n",
    "    **kwargs: additional parameters to be passed to `extract_func`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    data, target = None, None\n",
    "    \n",
    "    if extract_function is None:\n",
    "        def extract_function(**kw):\n",
    "            return (data, target, metadata)\n",
    "  \n",
    "    # Generate `data` and `target` info\n",
    "    data, target, metadata = extract_func(metadata=metadata, **kwargs)\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Processing lvq-pak data\n",
    "Let's convert the lvq-pak data (introduced in the last section) into into `data` and `target` vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $unpack_dir/lvq_pak-3.1  # Files are extracted to a subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_train = unpack_dir / 'lvq_pak-3.1' / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'lvq_pak-3.1' / 'ex2.dat'\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 $datafile_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `datafile_train` (`ex1.dat`) appears to consists of:\n",
    "* the number of data columns, followed by\n",
    "* a comment line, then\n",
    "* space-delimited data\n",
    "\n",
    "**Wait!** There's a gotcha here. Look at the last entry in each row. That's the data label. In the last row, however, we see that `#` is used as a data label (easily confused for a comment). Be careful handling this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 $datafile_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `datafile_test` (`ex2.dat`) is similar, but has no comment header.\n",
    " Let's parse these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True, metadata=None):\n",
    "    \"\"\"Read an space-delimited file\n",
    "    \n",
    "    Data is space-delimited. Last column is the (string) label for the data\n",
    "\n",
    "    Note: we can't use automatic comment detection, as `#` characters are also\n",
    "    used as data labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skiprows: list-like, int or callable, optional\n",
    "        list of rows to skip when reading the file. See `pandas.read_csv`\n",
    "        entry on `skiprows` for more\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class (target) label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_csv(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target, metadata = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lvq_pak(dataset_name='lvq-pak', metadata=None, kind='all'):\n",
    "    \"\"\"Process LVQ-data object\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: (string)\n",
    "        Name of this raw dataset. This will be used as a key for accessing this raw dataset in the\n",
    "        Raw Dataset catalog\n",
    "    metadata: dict or None\n",
    "        If None, an empty metadata dictionary will be used.\n",
    "    kind: {'train', 'test', 'all'}\n",
    "        Whether to return training set, test set, or everything. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing the following keys:\n",
    "        dataset_name: (string)\n",
    "            `dataset_name` that was passed to the function\n",
    "        metadata: (dict)\n",
    "            dict containing the input `metadata` key/value pairs, and (optionally)\n",
    "            additional information about this raw dataset\n",
    "        data: array-style object\n",
    "            Often a `numpy.ndarray` or `pandas.DataFrame`\n",
    "        target: (optional) vector-style object\n",
    "            for supervised learning problems, the target vector associated with `data`\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    untar_dir = interim_data_path / dataset_name\n",
    "    unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target, metadata = read_space_delimited(unpack_dir / 'ex1.dat',\n",
    "                                                      skiprows=[0,1],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'test':\n",
    "        data, target, metadata = read_space_delimited(unpack_dir / 'ex2.dat',\n",
    "                                                      skiprows=[0],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'all':\n",
    "        data1, target1, metadata = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1],\n",
    "                                                        metadata=metadata)\n",
    "        data2, target2, metadata = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0],\n",
    "                                                        metadata=metadata)\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'metadata': metadata,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "    }\n",
    "    return dset_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_lvq_pak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.load_function = process_lvq_pak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write this into the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_datasource(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsrc.process() # Use the load_function to convert this DataSource to a real Dataset\n",
    "str(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsrc.process(kind=\"test\")  # Should be half the size\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Turn the F-MNIST `DataSource` into a `Dataset`\n",
    "In the last exercise, you fetched and unpacked F-MNIST data.\n",
    "Now it's time to process it into a `Dataset` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Dataset` and Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tour of the Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complicated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Data: The Punchline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
